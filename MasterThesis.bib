@incollection{alexanderclarkHandbookComputationalLinguistics2010,
  title = {The {{Handbook}} of {{Computational Linguistics}} and {{Natural Language Processing}}: {{Introduction}}},
  booktitle = {The {{Handbook}} of {{Computational Linguistics}} and {{Natural Language Processing}}},
  author = {{Alexander Clark} and {Chris Fox} and {Shalom Lappin}},
  year = {2010},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781444324044.ch},
  pages = {1--8},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781444324044.ch},
  isbn = {978-1-4443-2404-4},
  keywords = {CL and NLP research,Clark's statistical parsing - probabilistic syntactic analysis of sentences in a corpus,compact representations of high level information eluding statistical models,comparing supervised and unsupervised grammar inference,complexity theory studies,computational linguistics (CL) field and NLP,computational resources in time and space computing elements of these classes,current methods employed in CL and NLP,formal language theory,handbook of computational linguistics and natural language processing (NLP),identifying classes of languages and their decidability,primary tools in CL and NLP - natural language properties and processes,shifting - to robust learning and processing systems in large corpora,statistical modeling,symbolic techniques}
}

@article{artetxeMassivelyMultilingualSentence2019,
  title = {Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond},
  author = {Artetxe, Mikel and Schwenk, Holger},
  year = {2019},
  journal = {Transactions of the association for computational linguistics},
  volume = {7},
  pages = {597--610},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}},
  urldate = {2025-01-10},
  file = {C:\Users\lpaal\Zotero\storage\7A3DE4AD\Artetxe and Schwenk - 2019 - Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.pdf}
}

@article{bauerWordFamilies1993,
  title = {Word Families},
  author = {Bauer, Laurie and Nation, Paul},
  year = {1993},
  journal = {International journal of Lexicography},
  volume = {6},
  number = {4},
  pages = {253--279},
  publisher = {Oxford University Press},
  urldate = {2025-03-17},
  file = {C:\Users\lpaal\Zotero\storage\J9AY5F9V\Bauer and Nation - 1993 - Word families.pdf}
}

@article{brownLanguageModelsAre2020,
  title = {Language Models Are Few-Shot Learners},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D. and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda},
  year = {2020},
  journal = {Advances in neural information processing systems},
  volume = {33},
  pages = {1877--1901},
  urldate = {2025-01-08},
  file = {C:\Users\lpaal\Zotero\storage\NJCDIF4A\Brown et al. - 2020 - Language models are few-shot learners.pdf}
}

@misc{ekstromSequentialRankAgreement2015,
  title = {Sequential Rank Agreement Methods for Comparison of Ranked Lists},
  author = {Ekstr{\o}m, Claus Thorn and Gerds, Thomas Alexander and Jensen, Andreas Kryger and {Brink-Jensen}, Kasper},
  year = {2015},
  month = aug,
  number = {arXiv:1508.06803},
  eprint = {1508.06803},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1508.06803},
  urldate = {2024-12-12},
  abstract = {The comparison of alternative rankings of a set of items is a general and prominent task in applied statistics. Predictor variables are ranked according to magnitude of association with an outcome, prediction models rank subjects according to the personalized risk of an event, and genetic studies rank genes according to their difference in gene expression levels. This article constructs measures of the agreement of two or more ordered lists. We use the standard deviation of the ranks to define a measure of agreement that both provides an intuitive interpretation and can be applied to any number of lists even if some or all are incomplete or censored. The approach can identify change-points in the agreement of the lists and the sequential changes of agreement as a function of the depth of the lists can be compared graphically to a permutation based reference set. The usefulness of these tools are illustrated using gene rankings, and using data from two Danish ovarian cancer studies where we assess the within and between agreement of different statistical classification methods.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\39TUVD5E\\Ekstr√∏m et al. - 2015 - Sequential rank agreement methods for comparison of ranked lists.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\N5RTDRNA\\1508.html}
}

@inproceedings{goldhahnBuildingLargeMonolingual2012,
  title = {Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: {{From}} 100 to 200 Languages.},
  shorttitle = {Building Large Monolingual Dictionaries at the Leipzig Corpora Collection},
  booktitle = {{{LREC}}},
  author = {Goldhahn, Dirk and Eckart, Thomas and Quasthoff, Uwe},
  year = {2012},
  volume = {29},
  pages = {31--43},
  urldate = {2025-01-05}
}

@article{heChoosingWordsTeach2019,
  title = {Choosing {{Words}} to {{Teach}}: {{A Novel Method}} for {{Vocabulary Selection}} and {{Its Practical Application}}},
  shorttitle = {Choosing {{Words}} to {{Teach}}},
  author = {He, Xuehong (Stella) and Godfroid, Aline},
  year = {2019},
  month = jun,
  journal = {TESOL Quarterly},
  volume = {53},
  number = {2},
  pages = {348--371},
  issn = {0039-8322, 1545-7249},
  doi = {10.1002/tesq.483},
  urldate = {2025-02-03},
  abstract = {Vocabulary learning materials and vocabulary learning research have a common objective of promoting effective vocabulary instruction (Schmitt, 2008), but in practice vocabulary learning materials tend to reflect materials writers' repertoire and intuition primarily (Tomlinson, 2011). In an effort to develop a stronger interface between research and practice, this article introduces a novel method for word selection based on words' frequency, usefulness, and difficulty (Laufer \& Nation, 2012). The researchers retrieved the frequency of 191 words and collocations targeted in a North American intensive English program from the Corpus of Contemporary American English (               COCA               ) and               COCA               -Academic, and collected usefulness and difficulty ratings from 76 experienced               ESL               instructors. Frequency correlated moderately with usefulness and difficulty, which supported the value of including usefulness and difficulty ratings as word selection criteria. A cluster analysis revealed five distinct groups of target words, which differed in frequency, usefulness, and difficulty. Teaching of the target words could be prioritized according to this sequence. This study introduces a step-by-step approach for materials writers, curriculum designers, and teaching professionals to identify word groupings in a potential list of target words, using a combination of objective and subjective data, with the prospect of creating more effective and more efficacious vocabulary learning materials.},
  langid = {english},
  file = {C:\Users\lpaal\Zotero\storage\PDY2J9NA\He and Godfroid - 2019 - Choosing Words to Teach A Novel Method for Vocabulary Selection and Its Practical Application.pdf}
}

@incollection{hunstonCorpusLinguistics2006a,
  title = {Corpus Linguistics},
  booktitle = {Encyclopedia of Language \& Linguistics (Second Edition)},
  author = {Hunston, S.},
  editor = {Brown, Keith},
  year = {2006},
  edition = {Second Edition},
  pages = {234--248},
  publisher = {Elsevier},
  address = {Oxford},
  doi = {10.1016/B0-08-044854-2/00944-5},
  abstract = {A corpus is an electronically stored collection of texts that is exploited using specialized software. Corpora are used to test hypotheses about language and to provide quantificational data about language use. In addition, they provide an insight into recurring patterns of language that are difficult to observe in other ways. As a result, theories of language based on corpus data are being developed. Corpora have also had an impact on a number of applications of linguistics, including language teaching and translation.},
  isbn = {978-0-08-044854-1},
  keywords = {(unit of) meaning,colligation,collocation,concordance,corpus/corpora,grammar,language teaching,lexical priming,lexis,pattern,phrase/phraseology,probability,translation,variation}
}

@book{jurafskySpeechLanguageProcessing2025,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2025},
  edition = {3rd},
  publisher = {(unpublished)}
}

@article{kendallNEWMEASURERANK1938b,
  title = {A {{NEW MEASURE OF RANK CORRELATION}}},
  author = {KENDALL, M. G.},
  year = {1938},
  month = jun,
  journal = {Biometrika},
  volume = {30},
  number = {1-2},
  pages = {81--93},
  issn = {0006-3444},
  doi = {10.1093/biomet/30.1-2.81}
}

@inproceedings{kentonBertPretrainingDeep2019,
  title = {Bert: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  shorttitle = {Bert},
  booktitle = {Proceedings of {{naacL-HLT}}},
  author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
  year = {2019},
  volume = {1},
  pages = {2},
  publisher = {Minneapolis, Minnesota},
  urldate = {2025-01-08},
  file = {C:\Users\lpaal\Zotero\storage\NDUBZTCS\Kenton and Toutanova - 2019 - Bert Pre-training of deep bidirectional transformers for language understanding.pdf}
}

@article{kokkinakisCorpusbasedApproachesCreation2011,
  title = {Corpus-Based Approaches for the Creation of a Frequency Based Vocabulary List in the {{EU}} Project {{KELLY}}--Issues on Reliability, Validity and Coverage},
  author = {Kokkinakis, Sofie Johansson and Volodina, Elena},
  year = {2011},
  journal = {Proceedings of eLex},
  volume = {2011},
  pages = {129--139},
  urldate = {2025-03-15},
  file = {C:\Users\lpaal\Zotero\storage\6RB3KWGD\Kokkinakis and Volodina - 2011 - Corpus-based approaches for the creation of a frequency based vocabulary list in the EU project KELL.pdf}
}

@phdthesis{leiInterpretableNeuralModels2017,
  type = {Thesis},
  title = {Interpretable Neural Models for Natural Language Processing},
  author = {Lei, Tao},
  year = {2017},
  urldate = {2025-02-26},
  abstract = {The success of neural network models often comes at a cost of interpretability. This thesis addresses the problem by providing justifications behind the model's structure and predictions. In the first part of this thesis, we present a class of sequence operations for text processing. The proposed component generalizes from convolution operations and gated aggregations. As justifications, we relate this component to string kernels, i.e. functions measuring the similarity between sequences, and demonstrate how it encodes the efficient kernel computing algorithm into its structure. The proposed model achieves state-of-the-art or competitive results compared to alternative architectures (such as LSTMs and CNNs) across several NLP applications. In the second part, we learn rationales behind the model's prediction by extracting input pieces as supporting evidence. Rationales are tailored to be short and coherent, yet sufficient for making the same prediction. Our approach combines two modular components, generator and encoder, which are trained to operate well together. The generator specifies a distribution over text fragments as candidate rationales and these are passed through the encoder for prediction. Rationales are never given during training. Instead, the model is regularized by the desiderata for rationales. We demonstrate the effectiveness of this learning framework in applications such multi-aspect sentiment analysis. Our method achieves a performance over 90\% evaluated against manual annotated rationales.},
  copyright = {MIT theses are protected by copyright. They may be viewed, downloaded, or printed from this source but further reproduction or distribution in any format is prohibited without written permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2017-05-11T19:59:27Z},
  file = {C:\Users\lpaal\Zotero\storage\4J9MB24F\Lei - 2017 - Interpretable neural models for natural language processing.pdf}
}

@book{liRoutledgeHandbookSecond2022,
  title = {The Routledge Handbook of Second Language Acquisition and Individual Differences},
  author = {Li, Shaofeng and Hiver, Phil and Papi, Mostafa},
  year = {2022},
  month = apr,
  publisher = {Routledge},
  doi = {10.4324/9781003270546},
  isbn = {978-1-032-21914-1}
}

@article{lisonOpensubtitles2016ExtractingLarge2016,
  title = {Opensubtitles2016: {{Extracting}} Large Parallel Corpora from Movie and Tv Subtitles},
  shorttitle = {Opensubtitles2016},
  author = {Lison, Pierre and Tiedemann, J{\"o}rg},
  year = {2016},
  journal = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  publisher = {European Language Resources Association},
  urldate = {2025-01-05},
  file = {C:\Users\lpaal\Zotero\storage\KP7TULVI\Lison and Tiedemann - 2016 - Opensubtitles2016 Extracting large parallel corpora from movie and tv subtitles.pdf}
}

@misc{liUnderstandingNeuralNetworks2017,
  title = {Understanding {{Neural Networks}} through {{Representation Erasure}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2017},
  month = jan,
  number = {arXiv:1612.08220},
  eprint = {1612.08220},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1612.08220},
  urldate = {2025-02-26},
  abstract = {While neural networks have been successfully applied to many natural language processing tasks, they come at the cost of interpretability. In this paper, we propose a general methodology to analyze and interpret decisions from a neural model by observing the effects on the model of erasing various parts of the representation, such as input word-vector dimensions, intermediate hidden units, or input words. We present several approaches to analyzing the effects of such erasure, from computing the relative difference in evaluation metrics, to using reinforcement learning to erase the minimum set of input words in order to flip a neural model's decision. In a comprehensive analysis of multiple NLP tasks, including linguistic feature classification, sentence-level sentiment analysis, and document level sentiment aspect prediction, we show that the proposed methodology not only offers clear explanations about neural model decisions, but also provides a way to conduct error analysis on neural models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\C5APXWAP\\Li et al. - 2017 - Understanding Neural Networks through Representation Erasure.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\QGKASWLY\\1612.html}
}

@misc{LlamamodelsModelsLlama3_3,
  title = {Llama-Models/Models/Llama3\_3/{{MODEL}}\_{{CARD}}.Md at Main {$\cdot$} Meta-Llama/Llama-Models},
  journal = {GitHub},
  urldate = {2025-01-08},
  abstract = {Utilities intended for use with Llama models. Contribute to meta-llama/llama-models development by creating an account on GitHub.},
  howpublished = {https://github.com/meta-llama/llama-models/blob/main/models/llama3\_3/MODEL\_CARD.md},
  langid = {english},
  file = {C:\Users\lpaal\Zotero\storage\R3979IAB\MODEL_CARD.html}
}

@inbook{manning84EvaluationRanked2008,
  title = {8.4 {{Evaluation}} of Ranked Retrieval Results},
  booktitle = {Introduction to Information Retrieval},
  year = {2008},
  publisher = {Cambridge University Press},
  address = {New York},
  collaborator = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  isbn = {978-0-521-86571-5},
  lccn = {QA76.9.T48 M26 2008},
  keywords = {Document clustering,Information retrieval,Semantic Web,Text processing (Computer science)},
  annotation = {OCLC: ocn190786122}
}

@book{manningIntroductionInformationRetrieval2008,
  title = {Introduction to Information Retrieval},
  author = {Manning, Christopher D. and Raghavan, Prabhakar and Sch{\"u}tze, Hinrich},
  year = {2008},
  publisher = {Cambridge University Press},
  address = {New York},
  isbn = {978-0-521-86571-5},
  lccn = {QA76.9.T48 M26 2008},
  keywords = {Document clustering,Information retrieval,Semantic Web,Text processing (Computer science)},
  annotation = {OCLC: ocn190786122}
}

@book{michaelwestGeneralServiceList1953,
  title = {A {{General Service List}} of {{English Words}}},
  author = {{Michael West}},
  year = {1953},
  publisher = {{Longman, Green and Co.}},
  address = {London}
}

@article{nationVocabularySizeText1997,
  title = {Vocabulary Size, Text Coverage and Word Lists},
  author = {Nation, P.},
  year = {1997},
  journal = {Vocabulary: Description, acquisition and pedagogy/acquisition and pedagogy},
  pages = {6--19},
  urldate = {2025-01-03},
  file = {C:\Users\lpaal\Zotero\storage\4RI9RSFC\nation_waring_97.html}
}

@misc{openaiGPT4TechnicalReport2024,
  title = {{{GPT-4 Technical Report}}},
  author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and {Bernadett-Shapiro}, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Sim{\'o}n Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and {Gontijo-Lopes}, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, {\L}ukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, {\L}ukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and M{\'e}ly, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cer{\'o}n and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
  year = {2024},
  month = mar,
  number = {arXiv:2303.08774},
  eprint = {2303.08774},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08774},
  urldate = {2025-01-08},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\4WXF44XZ\\citation-359924624.bib;C\:\\Users\\lpaal\\Zotero\\storage\\MS6P9TPW\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\3QXLC6QX\\2303.html}
}

@article{pellicerDataAugmentationTechniques2023,
  title = {Data Augmentation Techniques in Natural Language Processing},
  author = {Pellicer, Lucas Francisco Amaral Orosco and Ferreira, Taynan Maier and Costa, Anna Helena Reali},
  year = {2023},
  month = jan,
  journal = {Applied Soft Computing},
  volume = {132},
  pages = {109803},
  issn = {1568-4946},
  doi = {10.1016/j.asoc.2022.109803},
  urldate = {2025-01-05},
  abstract = {Data Augmentation (DA) methods -- a family of techniques designed for synthetic generation of training data -- have shown remarkable results in various Deep Learning and Machine Learning tasks. Despite its widespread and successful adoption within the computer vision community, DA techniques designed for natural language processing (NLP) tasks have exhibited much slower advances and limited success in achieving performance gains. As a consequence, with the exception of applications of back-translation to machine translation tasks, these techniques have not been as thoroughly explored by the wider NLP community. Recent research on the subject still lacks a proper practical understanding of the relationship between the various existing DA methods. The connection between DA methods and several important aspects of its outputs, such as lexical diversity and semantic fidelity, is also still poorly understood. In this work, we perform a comprehensive study of NLP DA techniques, comparing their relative performance under different settings. We analyze the quality of the synthetic data generated, evaluate its performance gains and compare all of these aspects to previous existing DA procedures.},
  keywords = {Back-translation,Data augmentation,Machine learning,Natural language processing},
  file = {C:\Users\lpaal\Zotero\storage\LRTRTDHQ\S1568494622008523.html}
}

@article{qaiserTextMiningUse2018,
  title = {Text Mining: Use of {{TF-IDF}} to Examine the Relevance of Words to Documents},
  shorttitle = {Text Mining},
  author = {Qaiser, Shahzad and Ali, Ramsha},
  year = {2018},
  journal = {International Journal of Computer Applications},
  volume = {181},
  number = {1},
  pages = {25--29},
  urldate = {2025-01-03}
}

@article{qaiserTextMiningUse2018a,
  title = {Text Mining: {{Use}} of {{TF-IDF}} to Examine the Relevance of Words to Documents},
  author = {Qaiser, Shahzad and Ali, Ramsha},
  year = {2018},
  month = jul,
  journal = {International Journal of Computer Applications},
  volume = {181},
  doi = {10.5120/ijca2018917395}
}

@article{radfordLanguageModelsAre2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  journal = {OpenAI blog},
  volume = {1},
  number = {8},
  pages = {9},
  urldate = {2025-01-08},
  file = {C:\Users\lpaal\Zotero\storage\44LNTG4C\Radford et al. - 2019 - Language models are unsupervised multitask learners.pdf}
}

@incollection{rajaraman2011data,
  title = {Data Mining},
  booktitle = {Mining of Massive Datasets},
  author = {Rajaraman, Anand and Ullman, Jeffrey D.},
  year = {2011},
  pages = {1--17},
  publisher = {Cambridge University Press}
}

@misc{reimersMakingMonolingualSentence2020,
  title = {Making {{Monolingual Sentence Embeddings Multilingual}} Using {{Knowledge Distillation}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2020},
  month = oct,
  number = {arXiv:2004.09813},
  eprint = {2004.09813},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.09813},
  urldate = {2025-01-13},
  abstract = {We present an easy and efficient method to extend existing sentence embedding models to new languages. This allows to create multilingual versions from previously monolingual models. The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence. We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model. Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training is lower. We demonstrate the effectiveness of our approach for 50+ languages from various language families. Code to extend sentence embeddings models to more than 400 languages is publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\YC6333GB\\Reimers and Gurevych - 2020 - Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\5QZMKGWF\\2004.html}
}

@book{rich1983artificial,
  title = {Artificial Intelligence},
  author = {Rich, E.},
  year = {1983},
  series = {Artificial Intelligence},
  publisher = {McGraw-Hill},
  isbn = {978-0-07-052261-9}
}

@article{savickyMeasuresWordCommonness2002,
  title = {Measures of {{Word Commonness}}},
  author = {Savick{\'y}, Petr and Hlav{\'a}cov{\'a}, Jaroslava},
  year = {2002},
  month = dec,
  journal = {Journal of Quantitative Linguistics},
  volume = {9},
  number = {3},
  pages = {215--231},
  issn = {0929-6174, 1744-5035},
  doi = {10.1076/jqul.9.3.215.14124},
  urldate = {2025-03-15},
  langid = {english}
}

@misc{schwenkCCMatrixMiningBillions2020,
  title = {{{CCMatrix}}: {{Mining Billions}} of {{High-Quality Parallel Sentences}} on the {{WEB}}},
  shorttitle = {{{CCMatrix}}},
  author = {Schwenk, Holger and Wenzek, Guillaume and Edunov, Sergey and Grave, Edouard and Joulin, Armand},
  year = {2020},
  month = may,
  number = {arXiv:1911.04944},
  eprint = {1911.04944},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1911.04944},
  urldate = {2025-01-10},
  abstract = {We show that margin-based bitext mining in a multilingual sentence space can be applied to monolingual corpora of billions of sentences. We are using ten snapshots of a curated common crawl corpus (Wenzek et al., 2019) totalling 32.7 billion unique sentences. Using one unified approach for 38 languages, we were able to mine 4.5 billions parallel sentences, out of which 661 million are aligned with English. 20 language pairs have more then 30 million parallel sentences, 112 more then 10 million, and most more than one million, including direct alignments between many European or Asian languages. To evaluate the quality of the mined bitexts, we train NMT systems for most of the language pairs and evaluate them on TED, WMT and WAT test sets. Using our mined bitexts only and no human translated parallel data, we achieve a new state-of-the-art for a single system on the WMT'19 test set for translation between English and German, Russian and Chinese, as well as German/French. In particular, our English/German system outperforms the best single one by close to 4 BLEU points and is almost on pair with best WMT'19 evaluation system which uses system combination and back-translation. We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2019 workshop on Asian Translation (WAT).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\B9BHRYBT\\Schwenk et al. - 2020 - CCMatrix Mining Billions of High-Quality Parallel Sentences on the WEB.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\R96A292X\\1911.html}
}

@article{spearmanCorrelationCalculatedFaulty1910,
  title = {Correlation Calculated from Faulty Data},
  author = {Spearman, Charles},
  year = {1910},
  journal = {British journal of psychology},
  volume = {3},
  number = {3},
  pages = {271},
  publisher = {Cambridge University Press},
  urldate = {2024-12-12}
}

@inproceedings{tiedemannOPUSMTbuildingOpenTranslation2020,
  title = {{{OPUS-MT}}--Building Open Translation Services for the World},
  booktitle = {Proceedings of the 22nd Annual Conference of the {{European Association}} for {{Machine Translation}}},
  author = {Tiedemann, J{\"o}rg and Thottingal, Santhosh},
  year = {2020},
  pages = {479--480},
  urldate = {2025-01-07},
  file = {C:\Users\lpaal\Zotero\storage\PDFTFL9X\Tiedemann and Thottingal - 2020 - OPUS-MT‚Äìbuilding open translation services for the world.pdf}
}

@article{vaswani2017attention,
  title = {Attention Is {{All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {Advances in neural information processing systems},
  volume = {30}
}

@article{viloneNotionsExplainabilityEvaluation2021,
  title = {Notions of Explainability and Evaluation Approaches for Explainable Artificial Intelligence},
  author = {Vilone, Giulia and Longo, Luca},
  year = {2021},
  journal = {Information Fusion},
  volume = {76},
  pages = {89--106},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2021.05.009},
  abstract = {Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models that lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested, coupled with several studies attempting to define the concept of explainability and its evaluation. This systematic review contributes to the body of knowledge by clustering all the scientific studies via a hierarchical system that classifies theories and notions related to the concept of explainability and the evaluation approaches for XAI methods. The structure of this hierarchy builds on top of an exhaustive analysis of existing taxonomies and peer-reviewed scientific material. Findings suggest that scholars have identified numerous notions and requirements that an explanation should meet in order to be easily understandable by end-users and to provide actionable information that can inform decision making. They have also suggested various approaches to assess to what degree machine-generated explanations meet these demands. Overall, these approaches can be clustered into human-centred evaluations and evaluations with more objective metrics. However, despite the vast body of knowledge developed around the concept of explainability, there is not a general consensus among scholars on how an explanation should be defined, and how its validity and reliability assessed. Eventually, this review concludes by critically discussing these gaps and limitations, and it defines future research directions with explainability as the starting component of any artificial intelligent system.},
  keywords = {Evaluation methods,Explainable artificial intelligence,Notions of explainability}
}

@misc{wangShapleyExplanationNetworks2021,
  title = {Shapley {{Explanation Networks}}},
  author = {Wang, Rui and Wang, Xiaoqian and Inouye, David I.},
  year = {2021},
  month = apr,
  number = {arXiv:2104.02297},
  eprint = {2104.02297},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.02297},
  urldate = {2025-01-03},
  abstract = {Shapley values have become one of the most popular feature attribution explanation methods. However, most prior work has focused on post-hoc Shapley explanations, which can be computationally demanding due to its exponential time complexity and preclude model regularization based on Shapley explanations during training. Thus, we propose to incorporate Shapley values themselves as latent representations in deep models thereby making Shapley explanations first-class citizens in the modeling paradigm. This intrinsic explanation approach enables layer-wise explanations, explanation regularization of the model during training, and fast explanation computation at test time. We define the Shapley transform that transforms the input into a Shapley representation given a specific function. We operationalize the Shapley transform as a neural network module and construct both shallow and deep networks, called ShapNets, by composing Shapley modules. We prove that our Shallow ShapNets compute the exact Shapley values and our Deep ShapNets maintain the missingness and accuracy properties of Shapley values. We demonstrate on synthetic and real-world datasets that our ShapNets enable layer-wise Shapley explanations, novel Shapley regularizations during training, and fast computation while maintaining reasonable performance. Code is available at https://github.com/inouye-lab/ShapleyExplanationNetworks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {C\:\\Users\\lpaal\\Zotero\\storage\\SWYJTPC4\\Wang et al. - 2021 - Shapley Explanation Networks.pdf;C\:\\Users\\lpaal\\Zotero\\storage\\VD999ALK\\2104.html}
}

@article{webberSimilarityMeasureIndefinite2010,
  title = {A Similarity Measure for Indefinite Rankings},
  author = {Webber, William and Moffat, Alistair and Zobel, Justin},
  year = {2010},
  month = nov,
  journal = {ACM Trans. Inf. Syst.},
  volume = {28},
  number = {4},
  pages = {20:1--20:38},
  issn = {1046-8188},
  doi = {10.1145/1852102.1852106},
  urldate = {2024-12-23},
  abstract = {Ranked lists are encountered in research and daily life and it is often of interest to compare these lists even when they are incomplete or have only some members in common. An example is document rankings returned for the same query by different search engines. A measure of the similarity between incomplete rankings should handle nonconjointness, weight high ranks more heavily than low, and be monotonic with increasing depth of evaluation; but no measure satisfying all these criteria currently exists. In this article, we propose a new measure having these qualities, namely rank-biased overlap (RBO). The RBO measure is based on a simple probabilistic user model. It provides monotonicity by calculating, at a given depth of evaluation, a base score that is non-decreasing with additional evaluation, and a maximum score that is nonincreasing. An extrapolated score can be calculated between these bounds if a point estimate is required. RBO has a parameter which determines the strength of the weighting to top ranks. We extend RBO to handle tied ranks and rankings of different lengths. Finally, we give examples of the use of the measure in comparing the results produced by public search engines and in assessing retrieval systems in the laboratory.},
  file = {C:\Users\lpaal\Zotero\storage\EXCWL67C\Webber et al. - 2010 - A similarity measure for indefinite rankings.pdf}
}

@misc{WhatMachineLearning,
  title = {What {{Is Machine Learning}} ({{ML}})? {\textbar} {{IBM}}},
  urldate = {2025-02-26},
  howpublished = {https://www.ibm.com/think/topics/machine-learning}
}
