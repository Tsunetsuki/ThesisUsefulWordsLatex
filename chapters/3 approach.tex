This chapter describes in detail our approach for word utility evaluation fulfilling the goal stated in chapter \ref{sec:statement-of-goal}.

The problem is first put into terms of mathematical objects in chapter \ref{sec:problem-statement-formal}.
We then describe a hypothetical experiment that could be performed to evaluate word utility using human feedback in chapter \ref{sec:human-efficiency-testing}
After pointing out the unfeasibility of such a testing method, we suggest ways to perform the experiment with AI models instead of humans as the test subject in chapter \ref{sec:ai-simulated-learner}, using the technological building blocks referred to previously in chapter \ref{sec:basic-nlp-concepts}.

With a method for utility evaluation established, we then point out why it is in itself not yet sufficient for actually generating useful lists of vocabulary that a human learner could utilize in chapter \ref{sec:eval-vs-creation}.
To solve this final problem, \ref{sec:utility-extraction-with-xai} puts forward a general approach for list generation.

\section{Problem Statement for Word Utility Evaluation} \label{sec:problem-statement-formal}

% the following terms have been defined before can should be used now
% language context
% utility
% proxy task

To repeat the introductory sentence of chapter \ref{sec:statement-of-goal}:
The aim of this work is to find words that have the maximum \textit{utility} given a particular \textit{language context} by means of \textit{proxy tasks}.
Keeping in mind that this is done in order to sort words into vocabulary lists that can be used by language learners, we can conclude that the output of a proposed solution to this problem would be an \textbf{ordered list of words}. The following formula gives a more precise idea of the aim and the variables involved.

Since utility is defined in terms of language ability, we first need a way to put a number on the language ability of a test subject. This is done by means of a proxy task: We can imagine a human test subject taking a language exam as a proxy task to find out their language ability:

\begin{align*}
	t: \text{Human} \to [0, 1] \\
	t (s) \mapsto p            \\
\end{align*}
where $s$ is the test subject and $t$ is the proxy task, with a possible score $p$ between zero and one.

We can imagine many variables going into this function, such as the time when the test is taken (hopefully the human's performance would increase over time). However, this thesis addresses vocabulary learning, and so the vocabulary of the test subject is provided as an additional parameter into the function

Let $W$ be the set of all words in the language.
\begin{align*}
	t: \text{Human}, 2^{W}\to \mathbb{R} \\
	t (s, V) \mapsto p                   \\
\end{align*}
where $V \in W$ is the vocabulary of the test subject.

We now have a function from the vocabulary to the score in the proxy task (the proxy metric for language ability).
Using the proxy task function, we can define a measure of efficiency of an ordered list of vocabulary $l$ containing elements from $W$.
The efficiency reflects how quickly a learner improves their language ability if they learn words in the order of the list, measured as a proportion $p_{aim}$ of the maximum achievable performance $p_{max} = t(s,W)$ (knowing all words in the language).
It is the number of words which must be learned from the vocabulary list to bring the performance above the threshold:

Let
\begin{align*}
	l        & := (w_1, w_2, \dots, w_n) \quad \text{where } w_i \in W \text{ and } w_i \neq w_j \text{ for } i \neq j. \\
	V_{l, k} & := \{w_i \mid i \leq k\}                                                                                 \\
	p_{aim}  & \in  [0, 1]                                                                                              \\
\end{align*}

Then
\begin{align*}
	e_{t}(s, l, p_{aim}) \mapsto ( -1 )  \cdot  \min\{ k \mid  t(s,  V_{l, k}) \geq p_{aim} \cdot t(s, W)\} \\
\end{align*}
The term contains a multiplication with $-1$ such that an efficient list achieves a higher efficiency.

And with this definition of efficiency, we can define the condition for an optimal vocabulary list given a particular $p_{aim}$:
\begin{align} \label{eq:opt-vocab-list}
	l_{opt, t} (s, p_{aim}) = \argmax_{l} e(s, l, p_{aim})
\end{align}

Finding vocabulary lists that approximate an optimal list according to formula \ref{eq:opt-vocab-list} is the aim which the rest of this work is dedicated to.


\section{Experimental Setup: Measuring Word Utility as Ability Improvement in Humans} \label{sec:human-efficiency-testing}

\contentdescription{state utility in terms of language ability improvement
	aim is not just to evaluate, but also efficiently find lists of vocab
	give problems that is faced when trying to test this approach with human test subjects: no repeatability, high costs
}

With the formula \ref{eq:opt-vocab-list}, we now have set our goal more concretely.
We are left with the question of how to actually make candidate vocabulary lists, and test them for efficiency.
Let us first consider how we could measure the efficiency of one vocabulary list of 3000 words:

We would need to find a human test subject who does not know any words of the target language at the outset.
We would then make the subject learn the words from the list one by one, and test their language ability in regular intervals to see if their desired threshold has been reached.
This disregards the fact that knowing all words in the language would not necessarily mean the performance would reach 1, but assuming a normal school test that is not an issue.

While this test setup might take a long time to complete depending on the desired threshold, it is still feasible.
But if we wish to analyze the same subject's performance learning from a second vocabulary list to compare the lists' efficiencies, there is an unavoidable issue:
The test subject cannot completely forget the words they have learned in the first run.
The experiment is thus not repeatable.
If we increase the number of test subjects so each vocabulary list is learned by a new subject, we introduce differences in capabilities between test subjects and the cost of the experiment also quickly increases.

However, this theoretical test setup could be feasible using a non-human test subject:
Using AI models and Explainable AI to analyze their interaction with language, we could not only evaluate, but even compile lists with drastically reduced costs.
I shall lay out this approach in the remaining sections of this chapter.

\section{AI-Simulated Learner} \label{sec:ai-simulated-learner}
\contentdescription{I view AI models as containers of language knowledge. We can perform studies on it as though it were human, gaining knowledge "from the viewpoint of an entity interacting with language"}

This work is interested in finding vocabulary lists that language learners can use to gain linguistic competence in their chosen field in the least possible time.
Chapter \ref{sec:problem-statement-formal} has specified how with the help of a proxy task, we can define this efficiency.
Chapter \ref{sec:human-efficiency-testing} has laid out a theoretical approach for how with human test subject, one could test the efficiency of a vocabulary list.
However, we have established that the experiment is not easy to set up consistently, as it cannot be repeated with the same test subject on different lists and a large pool of test subjects lowers the comparability of the scores.

To circumvent this issue, this work proposes the following core idea:
By replacing the human test subject with an AI model, it could be possible to execute the experiment described in chapter \ref{sec:human-efficiency-testing} consistently and economically:

In recent years AI models such as ChatGPT \tocite{ChatGPT}, Gemini, DeepSeek, BERT have become highly adept at fulfilling language-related tasks such as Language Modeling \tocite{LLMs}, Named Entity Recognition, Sentiment Detection.

It may be said that they possess an understanding of language in a behaviorist sense.

These AI models are trained on one or more specific NLP tasks.
If we run the model on a specific corpus and mask some of the words in the input, it is expected that the performance will decrease in comparison to the full input.
However, presumable some words will have a larger impact on the performance than others.
So to test the efficiency of a vocabulary list, we could let the AI model perform its NLP task on the corpus.
At the start, we mask all of the tokens in the input to simulate a language learner who is a complete beginner and thus knows no words in their target language.
We can then progressively unmask words in the input, and each time run the AI on the corpus to check the updated (and presumably improved) performance.
This will yield a plot of performance over unmasked words which will tell us how quickly the performance improves with unmasked words.
A plot with low initial performance increase will betray an inefficient vocabulary list, whereas a quickly increasing performance should correspond to an efficient vocabulary list.

This is a cheap option when compared to using a human test subject and unlike the human, the AI model has no memory of previous tests, meaning we can run the experiment consistently every time.

The experiment described above necessarily requires data to be performed, in the form of a corpus.
We can suppose that both the corpus and the NLP task performed by the model will influence how quickly the AI model's performance increases with a given vocabulary list:

For example, to perform Sentiment Detection, words relating to emotion such as "hate", "like", "amazing" will matter more than if the task is related to Information Extraction, such as Temporal Tagging.

The corpus will have a more obvious influence on the performance, since corpora differ in many aspects such as formality, topic, and format of the text, all of which may be closely influenced by the learning reason of a language learner.
Thus, by choosing an appropriate corpus, we can test performance on language that is similar to the language in which a learner wishes to improve their understanding.



Thus, with this model in mind, we can translate some of the concepts introduced in chapters \ref{sec:statement-of-goal} into technical components to see how we could implement a technical solution to the problem of analyzing the utility of words.
Table \ref{table:concept-implementation-correspondence} shows the correspondences between the concept and the technical component.

\begin{table}[ht]
	\centering
	\begin{tabularx}{\textwidth}{|X|X|}
		\hline
		\textbf{Abstract concept}        & \textbf{Implementation}            \\
		\hline
		Test subject                     & AI model                           \\
		\hline
		Language ability                 & Performance in NLP task            \\
		\hline
		Language context                 & Corpus                             \\
		\hline
		Analysis of language interaction & XAI method                         \\
		\hline
		Word utility                     & Impact of word on task performance \\
		\hline
	\end{tabularx}
	\caption{Correspondence of abstract concepts to parts of implementation.}
	\label{table:concept-implementation-correspondence}
\end{table}


\section{Evaluating vs. Making Lists of Useful Vocabulary} \label{sec:eval-vs-creation}
The previous chapter describes an approach for evaluating the efficiency of vocabulary lists for learning in a specific context.
While it can be seen as an improvement on the human setup because of its improved consistency and economy, it describes only the process of evaluation, not generation.
Actually finding optimal vocabulary lists is theoretically possible by testing every possible vocabulary list with all words in the language.

In practice, however, this would require an enormous amount of computational resources:

If we wish to find the most efficient list of 1,000 words, and we take the most frequent 20,000 words in the language as candidates for members of this list, there is a total number of $P(20000, 1000) = \frac{20000!}{19000!}$ possible vocabulary lists to be tested.
Since every one of these test consists of running the proxy task with a vocabulary which is a subset taken from the first $n$ elements where $n \in [0, 1]$, and all of the subsets will appear in the runs of many vocabulary lists, we can optimize this by running the task with every vocabulary with a cardinality between 0 and 1,000, which would be
$
\sum_{k=0}^{1000} \binom{20000}{k} \approx \frac{1}{2} \times 2^{20000} = 2^{19999}
$ runs of the proxy task.
.
This is still an unfeasible amount of computation. 

It is therefore clear that finding optimal vocabulary list is much too expensive to be done without approximation.
As approximations, we suggest XAI to extract information about the linguistic skills of AI models, and the following chapters illustrate this approach.

\section{Utility extraction with Explainable AI} \label{sec:utility-extraction-with-xai}
\contentdescription{Advances in XAI, explain various approaches.}

The previous chapters have shown the challenges in evaluating the efficiency of vocabulary lists with human test subjects, and suggested a way to use AI models in connection with NLP tasks as a way to make a consistent and repeatable approach.
The problem of generating the lists in the first place still remains, however.

The guiding question of this thesis is:
\textit{"Which words provide the most language understanding in a particular context?"}.
Simulating the learner with AI models in the way described in chapter \ref{sec:ai-simulated-learner} turns this question into:
\textit{"Which words improve the performance of the AI model in the NLP task for a given corpus?"}.

This question is very close to the questions that the field of Explainable AI seeks to answer.
A typical question of Explainable AI would be:
\textit{"Why for a given input, did the model arrive at its output?"}.
If the input takes the form of natural language, we can be more specific:
\textit{"Which words in the input made the model arrive at its output?"}

By using XAI and answering this questions across the entire corpus, we can find out which words are the most essential for the AI model to perform its task in the corpus.
If the words found in this way are close to those that would provide the most utility to a human learner as well, they could be used to create very efficient vocabulary lists as well.


We therefore propose a framework to extract useful words which has the following main components:
\begin{itemize}
	\item Corpus
	\item (Pretrained) AI model
	\item NLP task
	\item XAI method
\end{itemize}

In the implementation, we will present various choices for these components, and argue for some to be used over others, with the aim of making the utility estimated by the framework align as much as possible with utility to human learners.

%
% It is readily seen that these components are not independent of each other.
% Some completely determine the choice of another, while others limit the selection of the other components.
% 	[describe dependencies between components]
% Task $\rightarrow$ model $\rightarrow$ tokenizer $\rightarrow$ words.
%
% must investigate comparability of results later.

% (also corpus $\rightarrow$ task)
% \subsection{Preliminary Investigations of Integrity}
% The core idea this paper is to evaluate word utility by investigating a function (in the form of an AI model) that presumably represents some level of understanding of the language and checking which inputs (words) have the biggest influence on either the input or the model's internal state.
% To ensure the function does possess this understanding, it is necessary to first ensure that the output of the model corresponds reliably to the ground truth, in other words, to tests the performance of the model on the specific data that will later be used to investigate the model itself.
%
% \todo{A lot of prelim. test results, for example, first tests of NSP model on opensubs sentences show low reliability. Might also have to move this section to end (or in "results" chapter?)}
%


%
%
% \section{Experimental Setup with AI Technology}
% With the experimental setup described in chapter \ref{seq:human-efficiency-testing}, we established a way to test the efficiency of a vocabulary list with a human test subject.
% Now that we have introduced the idea of performing the experiment with an AI model instead, let us examine how such an approach would work:
% Chapter \ref{table:concept-implementation-correspondence} suggested ways that we could use technological components to stand in for abstract concepts.
%
%
% \todo{ restate math problem definition with components replaced by AI}
%
%

\section{TO BE SORTED OR DELETED}
Let us examine examples of how utility may be defined:
In the question "Would you like some coffee?", the most important part is "coffee".
If we replaced the entire sentence with "coffee?", the meaning becomes less clear, but it is still possible to guess what is meant.
If, however, we replace the sentence with "would?", there is so little information in the sentence that it becomes impossible to guess the meaning.
To identify which words are important to understand the question, we could ask a human to point them out.
However, this is not very quantifiable and the answer is likely to be influenced by bias.
We could try removing some of the words in the sentence and see if a human hearing the question can still answer appropriately.
While this removes some of the bias, there are still issues with this approach:
Going through all possible permutations of the sentence would take a great amount of time to perform, and the same test subject cannot process one permutation without being influenced by the past experiences:
If we go through "would?", "you?", "like?", "some?", "coffee?" in sequence, the test subject would have full knowledge of the sentence by the time the last item is asked.
These problems can be alleviated by using AI models: They are cheaper to perform tests on than humans, and can be employed in such a way as to answer the question without being influenced by the previous questions each time.

\todo{Explain approach where a model is trained on small set of words and performance is evaluated. But too costly.}


While the models achieving state-of-the-art performance (neural networks) are black boxes for the most part, they are easier to understand than human decisions and the field of Explainable AI has produced various approaches to gauge the importance of inputs to the model.
Explainable AI has the explanation of the decisions of AI models as its aim.
This means that when analyzing the decisions of AI models by reducing them to human-understandable rules or by observing which words are the most important for the AI to fulfill its tasks, we are not technically observing rules of objective truth, but only the behavior which the AI has learned to perform its task.
If we try to extract truthful knowledge about language from the AI, we are relying on the assumption that the AI has learned rules that correspond to linguistic reality.
However, in the case of state-of-the-art models, we know the performance of such models to meet certain standards, which supports the above assumption.

