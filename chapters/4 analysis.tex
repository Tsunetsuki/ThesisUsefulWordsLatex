\section{Evaluation of Approaches}
\subsection{Comparison of list similarities}
In order to compare the results provided by the various approaches, metrics are needed that can be consistently calculated across the different approaches.
While a human may be able to qualitatively analyze lists and gain a rough idea of their similarity, computed metrics provide an instantaneous (if simplified) outlook on similarities.
A metric shall be defined as a function which takes as parameters two word lists of equal length which are word lists ordered descendingly by supposed relevance, and outputs a real number giving either a distance or similarity between the lists.

\paragraph{Considerations of the choice of metric}
\begin{itemize}
	\item  \textbf{Metrics must be able to handle elements which occur in only one of the two lists.}
	      Thus, a metric which solely compares ranks of elements is not viable.
	\item  \textbf{Changes at the beginning of the lists have a greater impact on the metric output than changes further down.}
	      The rationale is that two lists which have large differences at the start should be regarded as more different than ones where words
	\item  \textbf{Changes in rank should be counted as more important if they occur further up the list.}
	      A word that is rank 1 in list A but rank 101 in list B says more about the similarity than if a word is rank 2000 in list A but rank 2100 in list B.
\end{itemize}

\subsubsection{Metrics used}

\begin{description}
	\item \textbf{Sequential rank agreement (modified)} \cite{ekstromSequentialRankAgreement2015}: This metric is based on the deviations of some subset of the lists in the upper ranks.
        It is important to note that this metric has an additional parameter "depth" which determines how many elements (from the top of the list) are considered.
        It is therefore more helpful to view its results at various depths.
        The original formula for this metric in the case of two lists is:
	      \[
		      a_{d} := a \text{from start to rank} d
	      \]

	      \[
              S_{d} := a_{d} \cup b_{d} 
	      \]

	      \[
              SRA_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \left( r_{b}(x) \right) - \left( r_{a}(x) \right) \right)}{|S_{d}|}
	      \]

	      where \(\lambda\) is a normalization factor ensuring that \(\max(SRA) = 1\).
	      In its proposed form, this metric can only compare lists which contain the same set of unique elements, just in different orders.
	      In order to make it work on lists where this is not the case, one can set the "rank" of nonexisting elements to a value greater than the length of the lists, such as \(2 |a|\).
	      Another drawback of the metric is that the standard deviation of two numbers does not depend on their absolute value, only their difference.
	      However, to satisfy number 3 of the stated requirements, we can take the deviation of the logarithm of the ranks instead of the deviation of the ranks themselves, resulting in the formula

	      \[
		      r^{\prime}(x) :=
		      \begin{cases}
			      \mathrm{rank}_{b}(x) & \text{if } x \in b, \\
			      2 \cdot |a|          & \text{otherwise.}
		      \end{cases}
	      \]

	      \[
              SRA^{mod}_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \log (r^{\prime}_{b}(x))-\log(r^{\prime}_{a}(x))\right)}{|S_{d}|}
	      \]

	      For this modified version, \(\lambda\) can be calculated with:

	      \[
		      \lambda = \frac{1}{SRA_{d}(a, a^{*})},
	      \]

	      where \(a^{*}\) is a list such that \(a \cap a^{*} = \emptyset\).
	      % \item \textbf{Average overlap / rank-biased overlap} \cite{webberSimilarityMeasureIndefinite2010}: Compares ranked list and puts more emphasis on the top of the list than at the bottom. However, assumes that the elements of both lists are the same, i.e., that all elements of list A are also in list B and vice versa.
	      %
	      % \item \textbf{11-point interpolated average precision} \cite{manningIntroductionInformationRetrieval2008}: Uses set metric precision (though may also use recall or F1 score on various subsets of the list (first 10\%, first 20\% etc.) and takes their geometric mean to arrive at a single number.
	      %       As each of the elevent numbers is calculated on the partial subset of the list's elements starting at the first element, this means that changes in the top of the list affect more of these numbers and thus have a larger impact on the final calculated mean.
	      %       One drawback of this method is that precision measurement within the 10\% interval only takes into account set membership, not the order of words:
	      %       For a list containing 10,000 words, the evaluated intervals are words in the index intervals $\left[ 0, 1000 \right), \left[ 0, 2000 \right), ... ,\left[ 0, 10000 \right)$.  Differences of order within the first 1000 words are thus ignored.

	\item \textbf{Discounted Cumulative Gain}:
	      This formula outputs a value between 0 and 1, with 1 being given if both lists are identical, 0 when they have no elements in common, and values in between when there is partial overlap between elements and/or their order is different.
	      ${DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}=rel_{1}+\sum _{i=2}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$
\end{description}

\[
	rel_{i} :=
	\begin{cases}
		\frac{1}{rank_{b}(el_{i}) + 1} & \text{if } el_{i} \in b \\
		0                              & \text{otherwise}
	\end{cases}
\]
\subsubsection{Metrics rejected}
\begin{description}
	\item \textbf{Kendall rank correlation} \cite{kendallNEWMEASURERANK1938b}: This metric is bounded between 0 and 1 and compares the ranks of the elements of two lists. However, it cannot handle elements that only occur in one of the two lists, and thus is not suitable for our purposes. It also does not distinguish between differences in the upper and lower parts of the lists.
	\item \textbf{Spearman's footrule} \cite{spearmanCorrelationCalculatedFaulty1910}: Rejected for the same reasons as Kendall rank correlation.
\end{description}


