This chapter gives context to estimating the utility of vocabulary.

We first go over how this work connects to research about vocabulary acquisition for second language learners in chapter \ref{sec:context-of-work}.

After this, we defining the exact aim of the work in chapter \ref{sec:statement-of-goal}.

The following three chapters explain the (partially overlapping) fields of Natural Language Processing, Machine Learning, and Explainable AI, tools from all of which will be employed in our word utility evaluation approach laid out in chapter \ref{ch:approach}.

Finally, we examine current approaches for selecting vocabulary for language learners in chapter \ref{sec:state-of-the-art-vocabulary-selection}, to show what methods are currently employed and how they could be improved.


\section{Context of the Work} \label{sec:context-of-work}
\contentdescription{language learning in general, cite reference works and textbooks. Maybe mention context-specific learning resources too, such as academic English}

\subsection{Linguistic motivation}
The issue of which words to learn when setting out to acquire a language is not a new one.
Every textbook on language which does not focus exclusively on grammar must ask address the issue, and proactive learners will doubtless find themselves finding ways to optimize the return on their invested studying time.

The Routledge Handbook of Second Language Acquisition \cite{liRoutledgeHandbookSecond2022} gives three criteria for deciding the order in which words are taught to beginners:

\begin{enumerate}
	\item Frequency
	\item Usefulness
	\item Easiness
\end{enumerate}

The first criterion, frequency, is easy to justify:
Learning words that appear with great frequency will allow the learner to understand the maximum percentage of wrods in texts, and should thus be among the most relevant.
The \textit{easiness}, or \textit{learnability} of a word is defined as how easy it is for a learner to acquire the word.
The authors define it with respect to an individual learner since it is posited that the various linguistic backgrounds of learners will influence which words they can acquire with ease.
Apart from learner-independent factors influencing the learnability of a word such as word length, is sure to have an impact whether the learners already knows a cognate of the word:

We can imagine a native German speaker attempting to learn English:
The word "internationalization" may not be a particularly frequent word in most contexts and thus not seem too important to teach, but the German will likely recognize the word as a cognate of the German equivalent "Internationalisierung", and acquire the word with ease, giving a justification for teaching the word early if we wish to impart the most language knowledge as quickly as possible.

Finally, there is the criterion of "\textit{usefulness}".
While it is distinguished from frequency, the authors do not define what constitutes usefulness, or how it might be measured.

In their 2019 paper \cite{heChoosingWordsTeach2019}, He and Godfroid pick up the above three criteria in order to group words into clusters which should help determine their priority in vocabulary learning.
However, again it is not defined what constitutes usefulness and no way to measure it is suggested beyond "human intuition".
In fact, it is put in contrast with frequency, which may be derived from corpus data, whereas usefulness cannot.

As the aim of this work is to find ways to evaluate word utility automatically, the following chapter will attempt to define utility and related concepts more carefully, in order to state the aim of this work more formally.

\subsection{Statement of Goal} \label{sec:statement-of-goal}
\contentdescription{Statement of goal, definition of "utility", context }

Generally speaking, this work addresses the problem of evaluating how useful it is to learn a given word in the target language of a language learner.
This helps us in generating ordered lists of vocabulary to learn which aid the learner in gaining competency in their target language quickly.
Since different language learners learn languages for different reasons \missingcitation{Statistics on motivations of language learners}, we attempt to take into account the learner's motivation to tailor vocabulary specifically to the individual.

To make this goal more specific, we define several concepts that help us speak more precisely about this goal and how it may be accomplished.
These concepts are used to throughout this work.

\subsubsection{Linguistic Context}
By a linguistic context or just \textit{context}, a subset of situations is meant in which a person interacts with language.
Examples of language contexts by topic are:
News, football, crocheting.

Examples of language contexts by situation are:
Everyday conversation, scientific articles, watching movies.

Contexts can be further subdivided into smaller contexts, or grouped across different lines:
"News" could be subdivided into "international politics", "sports", "business", etc.

The concept of a language context is useful to a language learner since it enables them to prioritize learning vocabulary and grammar associated with the context they are interested in:
Someone who is learning Arabic to better understand middle-eastern politics will have little use for words which are mostly associated with football.

The context could even determine the complexity of sentences that a learner must be able to understand:
Some contexts, like everyday conversation, can be navigated fairly well with short sentences, but political speeches often feature long and structurally complex sentences, with various relative clauses, multiple negations, etc. \missingcitation{Complexity differing across contexts}
Thus, learning for one or multiple specific contexts gives the learner a more concrete goal and thus better idea of what they must learn to achieve it.

In contemporary methods and tools for language learning, usually little emphasis is placed on learning for a specific context, or else few contexts are available \missingcitation{investigage current popular language tools. Classroom cannot cater to individual preferences and apps only allow some personalization}.

\subsubsection{Utility}
In this work, \textit{utility} is defined as the increase of language ability in a given context that a person gains by knowing a word versus not knowing it at all.
Here, language ability refers to being able to understand more of texts they read in the given context, being able to speak about the subject, etc.
It is easy to see that some words will be more useful than others given a context:
In the context of international news, learning the word "war" will bring more understanding to the learner than "pear" or "polymorphism".

\subsubsection{Proxy Task}
Some of the concepts in the realm of language learning cannot be measured directly:
This can be because they are psychological in nature ("understanding") or because they are too complex to be objectively measured by numbers ("language ability").
The above section defines word utility as an increase in "language ability", but to analyze this ability with computers, we must make be able to put a number on it, even though we lack both an agreed upon scale and unit of measurement.

To circumvent this issue, so-called \textit{proxy tasks} will be used:
Proxy tasks are tasks that test subjects can perform, where the test result (the \textit{proxy measure}) is used to capture the abstract quantity under study \missingcitation{Proxy task definition}.
For example, as a proxy measure for a person's general spelling ability, we could make them choose from multiple spelling variants of a word, and use the accuracy with which they select the correct answers.

This work employs AI models solving NLP tasks as an economic and repeatable substitute for real humans interacting with language, and uses the NLP tasks as proxy tasks to make "language understanding", and thus "utility", measurable concepts that can be computationally maximized.

\subsection{Summary}
With the concepts above defined more precisely, let us state the goal more accurately:

To find words that have the maximum \textit{utility} given a particular \textit{language context} by means of \textit{proxy tasks}.

With the words in cursive filled in, this translates to:
Finding words that provide a language learner with the most language understanding in the specific situations they are interested in, requiring them to learn the least possible amount of words.
Since the understanding of a learner cannot be directly measured, we use tasks that check the learner's understanding as an imperfect but necessary approximation.

The following chapters will go over the fields addressed in achieving this aim, and highlight relevant aspects from them that contribute in later chapters to conceptualizing and implementing a solution.


\section{Natural Language Processing} \label{sec:natural-language-processing}
While the aim of the work is to make vocabulary lists that serve human learners, the technical implementation of this necessarily will process human language with the use of computers.
This domain is called \textit{Natural Language Processing}, a field defined in The Handbook of Computational Linguistics and Natural Language Processing as the engineering domain of Computational Linguistics \cite{alexanderclarkHandbookComputationalLinguistics2010}.
Natural Language Processing, often abbreviated as \textit{NLP}, is both used in Computational Linguistics (the field concerned with analyzing natural language through language data) and as well as in practical applications such as Chatbots, Machine Translation, Speech Recognition etc. \cite{jurafskySpeechLanguageProcessing2025}.
This work addresses of these both aspects, as it is an undertaking of Computational Linguistics through the analysis of how AI models perform typical \textbf{NLP tasks}.

\subsection{Corpus}
The typical way Computational Linguistics empirically analyzes language is through the use of \textbf{corpora}.
A common definition of a corpus can be seen in \cite{hunstonCorpusLinguistics2006a} as "an electronically stored collection of samples of naturally occurring language", which "can be a test bed for hypotheses and can be used to add a quantitative dimension to many linguistic studies".

Recently, many large corpora have been compiled and are freely available to the public.
Corpora differ from each other mostly in their source and method of compilation.
Depending on their source, some corpora may serve as examples of language being used in a language context, such as news or movie subtitles.

In the implementation and evaluation of this work's approach to word utility estimation, several such context-specific corpora will be used, as they present a way to analyze how language (and more specifically, vocabulary) is used in the linguistic context of the corpus.
\todo{Quote previous work that uses corpora to model linguistic contexts for the purposes of language learning}

\subsection{NLP Task}
This work attempts to make use of not only static data in the form of corpora, but also the interaction of AI models with those corpora in order to gain insights into word utility.
This active part of \NLP\ consists of performing \textbf{NLP tasks}.
Typical NLP tasks include \cite{jurafskySpeechLanguageProcessing2025}

\begin{itemize}
	\item Sentiment Detection: Given a text, estimate the emotional state of the author.
	\item Masked Language modeling: Given a text with a word blanked out, estimate what the word should be.
	\item Machine Translation: Given a text, translate the meaning into another language while preserving meaning as faithfully as possible.
\end{itemize}


Humans are not trained from childhood to do any one language "task":
They can have conversations, answer questions about things they have seen, etc.
In contrast, the interaction of computers is usually more rigidly defined by specific NLP tasks, and AI models are trained to perform one or several of these tasks.

For the purposes of this work, an \textit{NLP task} is a function with a specific input and output format, where at least one of the two formats takes the form of natural language.
NLP tasks will serve as a way for AI models to interact with natural language, enabling us to analyze how certain words influence this interaction; the exact approach is described in chapter \ref{ch:approach}.


\subsection{NLP Preprocessing: Tokenization}
Language presents itself in continuous form in most situations:
When listening to spoken language, it is not obvious where one word ends and another begins.
Likewise, written texts we find online or in books are not necessarily subdivided into its semantic constituents.
While words in the written English language are mostly separated by spaces, a writer may choose to create a new hyphenated word sequence on the spot as necessity demands.

Further complicating the issue of where to separate words is the fact that many non-European language do not use spaces in their spelling (e.g. Japanese, Mandarin Chinese) or use spaces for a different purpose (separating syllables in Vietnamese, separating sentences in Thai).
For these reasons, \textit{tokenizers} are used in Natural Language Processing to divide continuous texts into individual words \cite{jurafskySpeechLanguageProcessing2025}.
\todo{Explain various types of tokenization: Sub-word, word etc.}

Splitting continuous text into distinct words will be required for this work and will serve the following purposes:
\begin{itemize}
	\item Making statistics from word counts (e.g., counting which words occur many times).
	\item Assigning estimated utility to words.
	\item Being able to remove individual words in text inputs to AI models, in order to test what effect removing a particular word has on the output.
\end{itemize}

In order to provide utility evaluation for words in a wide range of languages, this work will employ tokenizers which can handle differently language of different syntactic structures.

In recent years, major advances have been made in the field of Natural Language Processing through the use of AI models.
As this work makes extensive use of AI models, the next section therefore briefly introduces the field of \AI.

\section{Artificial Intelligence}
% \tocite{Wolfgang Ertel: Introduction to Artificial Intelligence} defines \AI\ as ""
Elaine Rich, in her 1983 work "Artificial Intelligence", defines \AI\ as "the study of how to make computers do things at which, at the moment, people are better" \cite{rich1983artificial}.
An introductory article by IBM describes it as "a technology that enables computers and machines to simulate human learning, comprehension, problem solving, decision making, creativity and autonomy" \footnote{\url{https://www.ibm.com/think/topics/artificial-intelligence}, last accessed February 26th, 2025}.
And this
This is opposed to traditional algorithms which can only follow algorithms which have been explicitly programmed.

\subsection{AI Models}
The building block of Artificial Intelligence is called an \textbf{AI Model}, which is a stochastic model using training data to learn patterns, and then used on inputs that were not identically present in the training dataset.

AI models are used in this work to simulate an agent interacting with language.
The (pre-trained) AI model is thought of as a test subject in possession of linguistic skills that can be studied.
Through its training process, AI models learn patterns that are generalizable across the problem domain.
If these patterns resemble the knowledge that humans gain when they learn a new ability, it may be possible to analyze the interaction of AI models with data so that a human learner can learn from their behavior, similar to how humans can observe experts in their domain and learn from their behavior.
The field which provides theses methods of analysis is Explainable AI, which is explained in chapter \ref{sec:explainable-ai}.

The current de facto standard for recent AI models is the \textit{Neural Network} architecture \cite{jurafskySpeechLanguageProcessing2025}, which attempts to imitate the neural makeup of the human brain.
Such models achieve state-of-the-art results on most NLP tasks \missingcitation{neural networks performing NLP task} and all models used in this work are Neural Networks for this reason.
However, within this architecture there exist further subtypes of AI models, the most important for this work is the \textbf{transformer model}.


\subsection{Transformer Model} \label{sec:transformer}
Since their invention in 2017 \cite{vaswani2017attention}, \textit{transformer} models have brought impressive gains in performance to many fields, including \NLP.

They are a particular type of deep neural network characterized by an attention layer before the fully connected neural layers.
Said attention layer makes the model "focus" on important parts of the important, while "ignoring" less important ones.
This helps the model find patterns in noisy input.
This attention is one possible way to attribute importance to inputs, and thus utility to words in our case.
This use will be discussed in chapter \ref{sec:xai-methods}.

This section has introduced the important AI models used in this thesis, namely the Neural Network and one of its most effective subtypes, the transformer.
While these types of AI model have driven many recent improvements in performing common NLP tasks, deep neural networks stop being readily understandable to humans rather quickly once the number of neurons and layers is increased.
We are interested in how the inputs (words) influence the outputs of AI models when performing NLP tasks.
This kind of analysis is the domain of \textbf{Explainable AI}, which is why the next section briefly introduces this last necessary field for the approach put forward in this work.

\section{Explainable AI} \label{sec:explainable-ai}
Explainable AI is the field of research focused on making the decision-making progress of AI models more transparent and understandable to humans, to enable us to reason about the AI model's decisions \cite{viloneNotionsExplainabilityEvaluation2021}.
This can be useful to check if the decision process contains social biases, or if it is based on wrong patterns learned from skewed training and test data (overfitting).
By analyzing the decision process of high-performing AI models, this work attempts to extract useful information about how the language is processed by them which may be useful to humans as well.
For this analysis, XAI is used a tool to find out which inputs influence the performance of AI models the most.

\subsection{Distinctions between XAI methods}
There are several categories of XAI methods that will be relevant in this work, because they determine not only the purpose of the method, but also which XAI method can be used on which AI model.
This chapter will go into some of distinctions and argue why some types are more appropriate to the aim of this thesis than others.

As was mentioned before, this work is interested in analyzing the influence of words in AI model input on its output for the purpose of finding out which words might be the most useful for human learners as well.
The question of which inputs are the most important for a model to reach its output is the domain of \textbf{feature importance explanations}, a sub-field of XAI concerned with the question: \textit{"how does each feature affect the model?"} \footnote{\url{https://courses.cs.washington.edu/courses/csep590b/22sp/files/lectures/lecture2_part1.pdf}, last accessed on February 27, 2025.}.
Thus, all XAI methods in this paper belong to this group.

Feature importance explanations can be further subdivided into two groups:
\textbf{Feature selection} methods explain the model's decisions by trying to predict a subset of all input features which are deemed "important".
\textbf{Feature attribution} methods attribute a concrete importance score $a_i$ $\in \mathbb{R}$ to each input feature $x_i$.
This work uses feature attribution methods exclusively, as the scores will allow us to rank words in a list by their importance.

Another important characteristics of an XAI method is whether it is \textbf{model-agnostic} or \textbf{model-specific}.
A model-agnostic XAI method can be used no matter what AI model is used, whereas a model-specific method is limited to being used to a specific type of AI model, such as transformers, decision trees \missingcitation{decision tree}, or neural networks.
Model-agnostic models are also called black-box-approaches, as they do not look into the model (such as weights in neural networks) but only perturb the inputs and observe changes in the model output.
A advantage of model agnostic explanations is that they can be used on any AI model
However, many recent state-of-the-art AI models are build using the transformer architecture, hence methods specific to transformer models can also be fairly broadly applied.
A major upshot of model-specific explanations is their efficiency:
Black-box approaches require us to perturb the input to observe differences in the output, requiring us to run the model multiple times for a single explanation, which can constitute a great computational effort.
On the other hand, model-specific methods typically only require one run of the model, as they can look into the model itself for an explanation.
For these reasons, both model-agnostic and model-specific approaches will be used in this work.

Another feature of XAI methods is the scope of its explanations:
\textbf{Local} XAI methods explain why a model output was generated for one particular input.
A \textbf{global} method attempts to reason about the model's behavior in general, independent of any particular input data.
This work is interested in observing the interaction of the AI model with a particular corpus, not only the AI model.
For this reason, all methods used in this paper will be local methods.
However, while local explanations only explain a single decision of the model (which, in the work, will be the interaction of the model with a single sentence), we will aggregate the individual decisions across the corpus to get a bigger picture about this interaction.

The transformer attention mechanism introduced in chapter \ref{sec:transformer} has been used as a model-specific XAI method
\cite{liUnderstandingNeuralNetworks2017}
\cite{leiInterpretableNeuralModels2017}
, and will be used in this work as one among several XAI methods employed to gauge the impact of words in the input to the output of (transformer) AI models.
The use of attention as explanation, too, will be discussed in more detail when describing the implementation of our approach in chapter \ref{sec:xai-methods}.

Table \ref{table:XAi-method-criteria} summarizes the used of XAI in this work.

\begin{table}[ht]
	\centering
	\begin{tabularx}{\textwidth}{|X|X|X|}
		\hline
		\textbf{Attribute} & \textbf{Type Selected for this work} & \textbf{Reason for selection} \\
		\hline
		\makecell[l]{Model-Specificity                                                            \\(Model-Agnostic or \\Model-Specific)} & Both Model-Agnostic and Model-Specific & Model-Agnostic methods: Broad applicability; Model-specific methods: Computational efficiency \\
		\hline
		\makecell[l]{Importance Explanation                                                       \\(Feature attribution or \\Feature selection)}                                 & Feature Attribution                 & Use of scores to order words in vocabulary lists                                                                        \\
		\hline
		\makecell[l]{Scope                                                                        \\(Local or Global)}                                                       & Local                                  & Attain explanations taking into account the corpus                                          \\
		\hline
	\end{tabularx}
	\caption{XAI methods used in this work}
	\label{table:XAi-method-criteria}
\end{table}

\section{State of the Art (of Vocabulary Selection)} \label{sec:state-of-the-art-vocabulary-selection}
\subsection{Non-computational Methods}
How is vocabulary order selected by current language teaching tools:
Not context-specific in most cases
offers only one order,
investigate if there are any automatic approaches besides frequency]

[can present methods for extracting useful words from text]
can be similar in either method or goal.
goal is finding useful words for language learning
method is extracting important words from text via XAI

\cite{heChoosingWordsTeach2019} cites concepts of frequency, usefulness and difficulty of words as widely accepted criteria to decide when to teach it.
However, the literature is not clear on how usefulness is defined outside of "human intuition" \cite{heChoosingWordsTeach2019}.
Furthermore, usefulness is presented as independent from frequency, making it more unclear still how it may be defined.
The advantage of human intuition is that humans can understand the nuances of texts better than computers, especially before the invention of large AI models tackling NLP tasks.
Relying on human intuition to evaluate word utility has two main drawbacks compared to computational methods:
\begin{itemize}
	\item Difficult to put a concrete number on word.
	\item Evaluating many languages would necessitate human experts for each language, necessitating expensive studies.
\end{itemize}

\subsubsection{Tiered Approach}
We can see one non-computational approach of vocabulary selection that Reading Rockets publishes on their website.
Reading Rockets \footnote{\url{https://www.readingrockets.org/}} is an American national public media literacy initiative by WETA Washington, D.C. \footnote{\url{https://weta.org/}}, a television station of the non-commercial public broadcaster Public Broadcasting Service \footnote{\url{https://www.pbs.org/}}.

As such, one of its aims is to help find reading materials appropriate for teaching children of school age words they are not yet familiar with, an aim similar to teaching second language learners new vocabulary.
Reading Rockets has made public several recommendation for how to find useful vocabulary to teach children as well, with the intention of helping individual teachers select vocabulary for their students. 
For this purpose, Reading Rockets uses a tiered system to sort vocabulary items into three tiers:
\footnote{\label{footnote:reading-rockets-choosing-words-to-teach} \url{https://www.readingrockets.org/topics/vocabulary/articles/choosing-words-teach}, last accessed on March 15, 2025.}
Tier One items are words which most children know before even entering primary school and thus need not be taught.
This includes words such as \textit{happy} and \textit{baby}.

Tier Three items are words which are useful mostly in one specific domain, such as \textit{peninsula} or \textit{lathe}.
These are not high frequency words, and thus Reading Rockets conclude that these words are best learned not in isolation, but rather "when needed in a content area".

Tier Two words are fairly high frequency words that are not so common as to be know by all children, but still useful across many domains, such as \textit{coincidence}, \textit{absurd}.
For this reason, Reading Rockets deems that "instruction in these words can add productively to an individual’s language ability." \footnoteref{footnote:reading-rockets-choosing-words-to-teach}

This system of triage can be useful and could be made to fit with the aim of teaching second-language learners too, by teaching Tier One words first, then moving on to Tier Two and Tier Three words.
Because the system proposed by Reading Rockets is a manual one, it requires some effort on the side of a teacher who would like to apply the system to their classroom.

In the next chapter, we will discuss various vocabulary selection methods that utilize analysis of existing textual data to arrive at vocabulary lists.

\subsection{Computational Methods}
These methods generate vocabulary lists by using corpora and computer-aided language processing to compile vocabulary lists.
They are closer related to the methods that this work strives for, and will be used as points of comparison when evaluating our own approaches.

\begin{description}
	\item [Raw frequency of words in corpus]
	      A simple ordering of words by how often they appear in a corpus.
	\item [Frequency with stopwords filtered out]
	      The same as frequency, but filtering out known stopwords from the resulting lists.
	\item [TF-IDF]
	      How often the words appear in a target document but divided by their frequency in a more generic corpus.
	      This metric is typically used to employ the most relevant words in documents for identifying keywords that express best its core topic.
	\item [Average Reduced Frequency]

	      Seems to be first brought up in \cite{savickyMeasuresWordCommonness2002},
	      used for vocab list compilation in \cite{kokkinakisCorpusbasedApproachesCreation2011}
	      and used by SketchEngine for tools of analysis \url{http://www.sketchengine.co.uk/} along with raw and relative frequency.
\end{description}

\subsection{Issues with Current Methods}
Current methods do not exploit recent developments in AI technology and thus suffer from several shortcomings:
In general, all of them essentially only count words without taking into account their relationship between each other:

Frequency: The most frequent words in texts are often words that carry little meaning by themselves, such as "a", "the", "of" in English.
While these may appear in many texts, they are not useful in determining their meaning.
TF-IDF: This metric has been used successfully to find the words that give the best hints at a text's topic.
However, it does not take into account and semantic relationships between the words in a text.
Thus, learning words by aggregating TF-IDFs on multiple texts may aid in identifying the topic of texts, but not at finding out what the message conveyed about the topic is.
"not" is a highly frequent word in English and thus will have a low TF-IDF score in most documents. But it is essential to know, as it can completely invert the meaning of a sentence.


