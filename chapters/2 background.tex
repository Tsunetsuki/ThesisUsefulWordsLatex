This chapter gives context to estimating the utility of vocabulary.

For this, we start by defining the exact aim of the work  in chapter \ref{seq:statement-of-goal}.
We then go over how this goal links to research about vocabulary acquisition for second language learners in chapter \ref{seq:context-of-work}.

We then examine current approaches for selecting vocabulary for language learners in chapter \ref{seq:state-of-the-art-vocabulary-selection}, to show what methods are currently employed and how they could be improved.

Finally, chapter \ref{seq:basic-nlp-concepts} explains some of the basic concepts of Natural Language Processing that are employed in this work to perform those improvements.



\section{Statement of goal} \label{seq:statement-of-goal}
\contentdescription{Statement of goal, definition of "utility", context }

Generally speaking, this work addresses the problem of evaluating how useful a vocabulary word is to learn in the target language of a language learner.
This helps us in generating ordered lists of vocabulary to learn which aid the learner in gaining competency in their target language quickly.
Since different language learners learn languages for different reasons, we take into account the learner's motivation to tailor vocabulary specifically to the individual.

To make this goal more specific, we define several concepts that help us speak more precisely about this goal and how it may be accomplished.
These concepts are used to throughout this work.
\subsection{(Language) Context}
By a \textit{language context} or just \textit{context}, a subset of situations is meant in which a person interacts with language.
Examples of language contexts by topic are:
News, football, crocheting.

Examples of language contexts by situation are:
Everyday conversation, scientific articles, watching movies.

Contexts can be further subdivided into smaller contexts, or grouped across different lines:
"News" could be subdivided into "international politics", "sports", "business", etc.

The concept of a language context is useful to a language learner since it enables them to prioritize learning vocabulary and grammar associated with the context they are interested in:
Someone who is learning Arabic to better understand middle-eastern politics will have little use for words which are mostly associated with football.

The context could even determine the complexity of sentences that a learner must be able to understand:
Some contexts, like everyday conversation, can be navigated fairly well with short sentences, but political speeches often feature long and structurally complex sentences, with various relative clauses, multiple negations, etc.
Thus, learning for one or multiple specific contexts gives the learner a more concrete goal and thus better idea of what they must learn to achieve it.

In contemporary methods and tools for language learning, usually little emphasis is placed on learning for a specific context, or else few contexts are available \tocitecontent{investigage current popular language tools. Classroom cannot cater to individual preferences and apps only allow some personalization}.

\subsection{Utility}
I define as \textit{utility} the increase of language ability in a given context that a person gains by knowing a word versus not knowing it at all.
This means being able to understand more of texts they read in the given context, being able to speak about the subject, etc.
It is easy to see that some words will be more useful than others given a context:
In the context of international news, learning the word "war" will bring more understanding to the learner than "pear" or "polymorphism".

\subsection{Proxy Task}
Some of the concepts in the realm of language learning cannot be measured directly:
This can be because they are psychological in nature ("understanding") or because they are too complex to be objectively measured by numbers ("language ability").
I define word utility above as an increase in "language ability", but to analyze this ability with computers, we must make be able to put a number on it, even though we lack both an agreed upon scale or unit of measurement.

To circumvent this issue, so-called \textit{proxy tasks} will be used:
Proxy tasks are tasks that test subjects can perform, where the test result (the \textit{proxy measure}) can be used to capture the abstract quantity under study.
For example, as a proxy measure for a person's general spelling ability, we could make them choose from multiple spelling variants of a word, and use the accuracy with which they select the correct answers.

This work employs AI models solving NLP tasks as an economic and repeatable substitute for real humans interacting with language, and uses the NLP tasks as proxy tasks to make "language understanding", and thus "utility", measurable concepts that can be computationally maximized.

\subsection{Restatement of goal}
With the concepts above defined more precisely, it becomes more concrete what our goal entails:

Finding words that have the maximum \textit{utility} given a particular \textit{language context} by means of \textit{proxy tasks}.

With the words filled in, this translates to:
Finding words that provide a learner with the most understanding in the learning context they are interested in in the minimum amount of words to learn.
Since the understanding of the learner cannot be directly measured, we use tasks that check the learner's understanding as an imperfect but necessary approximation.



\section{Context of the Work} \label{seq:context-of-work}
\contentdescription{language learning in general, cite reference works and textbooks. Maybe mention context-specific learning resources too, such as academic English}

The issue of which words to learn when setting out to acquire a language is not a new one.
Every textbook on language which is not completely grammatical must ask address the issue, and proactive learners will doubtless find themselves finding ways to optimize the return on their invested studying time.

The Routledge Handbook of Second Language Acquisition \tocite{The Routledge Handbook of Second Language Acquisition} gives three criteria for deciding the order in which words are taught to beginners:
\begin{enumerate}
	\item Frequency
	\item Usefulness
	\item Easiness
\end{enumerate}
The first criterion, frequency, is easy to justify:
Learning words that appear with great frequency will allow the learner to understand the maximum percentage of wrods in texts, and should thus be among the most relevant.
The \textit{easiness}, or \textit{learnabiility} of a word is defined as how easy it is for a learner to acquire the word.
The author define it with respect to a given learner.
Factors influencing the learnability of a word are word length, and whether the learners already knows a cognate of the word:
We can imagine a native German speaker attempting to learn English:
The word "internationalization" may not be a particularly frequent word in most contexts, but the German will likely recognize the word as a cognate of the German equivalent "Internationalisierung", and acquire the word with ease.

Finally, there is the criterion of \textit{usefulness}.
While it is distinguished from frequency, it is not defined what constitutes usefulness.

\tocite{ Choosing Words to Teach: A Novel Method for Vocabulary Selection and Its Practical Application } against picks up the above three criteria in order to group words into clusters which can be used to determine priority in vocabulary learning.

However, again it is not defined what constitutes usefulness suggests no way to measure it beyond "human intuition".
In fact, it is put in contrast with frequency, which may be derived from corpus data, whereas usefulness cannot.

This works attempts to derive usefulness from corpus data, in combination with AI models and XAI methods.



\section{State of the Art (of Vocabulary Selection)} \label{seq:state-of-the-art-vocabulary-selection}
\subsection{Non-computational Methods}
How is vocabulary order selected by current language teaching tools:
Not context-specific in most cases
offers only one order,
investigate if there are any automatic approaches besides frequency]

[can present methods for extracting useful words from text]
can be similar in either method or goal.
goal is finding useful words for language learning
method is extracting important words from text via XAI

\cite{heChoosingWordsTeach2019} cites concepts of frequency, usefulness and difficulty of words as widely accepted criteria to decide when to teach it.
However, the literature is not clear on how usefulness is defined outside of "human intuition" \cite{heChoosingWordsTeach2019}.
Furthermore, usefulness is presented as independent from frequency, making it more unclear still how it may be defined.
The advantage of human intuition is that humans can understand the nuances of texts better than computers, especially before the invention of large AI models tackling NLP tasks.
Relying on human intuition to evaluate word utility has two main drawbacks compared to computational methods:
\begin{itemize}
	\item Difficult to put a concrete number on word.
	\item Evaluating many languages would necessitate human experts for each language, necessitating expensive studies.
\end{itemize}

\subsection{Computational Methods}
These methods generate vocabulary lists by using corpora and computer-aided language processing to compile vocabulary lists.
They are closer related to the methods that this work strives for, and will be used as points of comparison when evaluating our own approaches.

\begin{description}
	\item [Raw frequency of words in corpus]
	      A simple ordering of words by how often they appear in a corpus.
	\item [Frequency with stopwords filtered out]
	      The same as frequency, but filtering out known stopwords from the resulting lists.
	\item [TF-IDF]
	      How often the words appear in a target document but divided by their frequency in a more generic corpus.
	      This metric is typically used to employ the most relevant words in documents for identifying keywords that express best its core topic.
\end{description}

\subsection{Issues with Current Methods}
Current methods do not exploit recent developments in AI technology and thus suffer from several shortcomings:
In general, all of them essentially only count words without taking into account their relationship between each other:

Frequency: The most frequent words in texts are often words that carry little meaning by themselves, such as "a", "the", "of" in English.
While these may appear in many texts, they are not useful in determining their meaning.
TF-IDF: This metric has been used successfully to find the words that give the best hints at a text's topic.
However, it does not take into account and semantic relationships between the words in a text.
Thus, learning words by aggregating TF-IDFs on multiple texts may aid in identifying the topic of texts, but not at finding out what the message conveyed about the topic is.
"not" is a highly frequent word in English and thus will have a low TF-IDF score in most documents. But it is essential to know, as it can completely invert the meaning of a sentence.

\section{Basic Concepts of Natural Language Processing} \label{seq:basic-nlp-concepts}
\contentdescription{
	[basic concepts, and why they matter to this work.
			Anything that that the target audience is either not familiar with, or where the function in my method is not obvious]
}
This section explains some basic concepts of Natural Language Processing domain, and how they relate to the goal of finding useful vocabulary.


	\subsection {AI Model}
	      While it is assumed the reader is familiar with the concept of AI models, in this work they serve a purpose that might not be obvious:
	      The (pre-trained) AI model is thought of as a test subject in possession of linguistic skills that can be studied.
	      The point is to analyze its skills and make them serviceable to a language learner, similar to how humans can observe experts in their domain and learn from their behavior.

	\subsection{Corpus}
	      A \textit{corpus} is simply a dataset consisting of text data.
	      Recently, many large corpora have been compiled and are freely available to the public.
	      Corpora differ from each other mostly in their source and method of compilation.
	      Depending on their source, some corpora may serve as examples of language being used in a language context, such as news or movie subtitles.
	      In the implementation and evaluation of this work's approach to word utility estimation, several such context-specific corpora will be used.

	\subsection{NLP Task}
	      Humans are generally intelligent and are not trained from childhood on any specific language task exclusively.
	      In contrast, AI models are trained to perform one or several specific tasks.
	      A \textit{NLP task} is a function with a specific input and output format, where at least one of the two formats takes the form of natural language.
	      Typical NLP tasks include:

	      \begin{itemize}
		      \item Sentiment detection: Given a text, estimate the emotional state of the author.
		      \item Masked Language modeling: Given a text with a word blanked out, estimate what the word should be.
		      \item Machine translation: Given a text, translate the meaning into another language while preserving meaning as faithfully as possible.
	      \end{itemize}

	      In this work, NLP tasks serve as a way for AI models to interaction with language, enabling us to analyze the interaction.

	\subsection{Explainable AI}
	      Deep neural networks (the recent standard for AI models) stop being readily understandable to humans rather quickly once the number of neurons and layers is increased.
	      Explainable AI is the field of research focused on making the decision-making progress of AI models more transparent and understandable to humans, to enable us to reason about the AI model's decisions. \tocite{Notions of explainability and evaluation approaches for explainable artificial intelligence, Interpretable machine learning: A guide for making black box models explainable}
	      This can be useful to check, if the decision process contains social biases, or if it is based on wrong patterns learned from skewed training and test data (overfitting).
	      By analyzing the decision process of high-performing AI models, we hope to extract useful information about how the language is processed which can be useful to humans as well.

	\subsection{Transformer Attention Mechanism}
	      Many of the recent state-of-the-art AI models performing NLP tasks are built with the \textit{transformer} architecture.
	      \tocite{Attention is all you need}
	      This is a particular type of deep neural network characterized by an attention layer before the deep neural layers.
	      Said attention layer makes the model "focus" on important parts of the important, while "ignoring" less important ones.
	      This helps the model find patterns in noisy input.
	      \tocite{Attention is all you need}
	      Attention has been used as one way to make decisions of AI models interpretable \tocite{Understanding Neural Networks through Representation Erasure} \tocite{Interpretable Neural Models for Natural Language
		      Processing}.
	      Thus, we will use it as one among several approaches to extract functional knowledge from NLP AI models.

	\subsection{Tokenizer}
	      Language presents itself in continuous form in most situations:
	      When listening to spoken language, it is not obvious where one word ends and another begins.
	      Likewise, written texts we find online or in books are not necessarily subdivided into its semantic constituents.
	      While words in the written English language are mostly separated by spaces, a writer may choose to create a new hyphenated word sequence on the spot as necessity demands.

	      Further complicating the issue of where to separate words is the fact that many non-European language do not use spaces in their spelling (e.g. Japanese, Mandarin Chinese) or use spaces for a different purpose (separating syllables in Vietnamese, separating sentences in Thai).
		  For this reason, \textit{tokenizers} are used in Natural Language Processing to divide continuous texts into their words. \tocite{Daniel Jurafsky \& James H. Martin: Speech and Language Processing}
		  \todo{Explain various types of tokenization?}

	      Splitting continuous text into distinct words has several benefits:
	      \begin{itemize}
		      \item we can make statistics from them (e.g., counting which words occur many times).
		      \item we can assign values to them, such as estimated utility.
		      \item we can mask them in text inputs to AI models to test what effect masking a particular word has on the output.
	      \end{itemize}

The following chapter lays out in detail how this work uses these components to work together to create approaches for word utility estimation.
