% \contentdescription{{ \begin{itemize}
% 				\item Sentence importance evaluation
% 				\item Capturing text as coherent unit instead of sentences which are independent of each other
% 				\item Active vs passive utility: Ideas for active utility estimation
% 				\item Compound splitting
% 				\item Lemmatization
% 				\item n-grams
% 				\item Multilinguality
% 			\end{itemize}
% 		} }
%
We have seen in the previous chapter that our approach to list generation outperforms frequency-based metrics when \todo{insert exact circumstances}.
This final chapter discusses several possible improvements to our approach and the research around it, which were not possible within the scope of this work due to time constraints.

\section{Lemmatization}
The vocabulary lists we compiled were dictated by the tokens provided by the tokenizer.
With this approach, each unique combination of letters is regarded as an independent word, including variations of a particular word such as "house" and "houses", "look", "looks", "looked".
However, in textbooks, vocabulary lists are usually organized by headwords or \textit{lemmas}, that is, the variations as seen above are considered as a single word.
To compile more useful vocabulary lists, we recommend that words which derive from the same lemma be treated as the same word.
"Treating" here includes the masking of tokens for model-agnostic XAI methods, as well grouping words together by their lemma in the post-processing of vocabulary lists which are produced by model-specific approaches.

\section{Active vs. Passive Utility}
This work has exclusively focused on the utility of words for understanding existing texts, however, one can also consider the utility of a word for active speaking and writing:
One obvious difference is that, when there are two synonyms, it suffices to know one of the two for active speaking, but for passive understanding, both must be learned.

\section{Multilinguality}

\section{Distinction of Homonyms}
