We have seen in the previous chapter that our approach to list generation outperforms frequency-based metrics on small vocabulary sizes, and especially when using short texts as inputs. 
This final chapter discusses several possible improvements to our approach, which were not possible within the scope of this work due to time constraints.

\section{Lemmatization}
The vocabulary lists we compiled were dictated by the tokens provided by the tokenizer.
With this approach, each unique combination of letters is regarded as an independent word, including variations of a particular word such as "house" and "houses" or "look", "looks", "looked".
However, in textbooks, vocabulary lists are usually organized by headwords or \textit{lemmas}, that is, the variations as seen above are considered as a single word.
To compile more useful vocabulary lists, we recommend that words which derive from the same lemma be treated as the same word.
"Treating" here includes the masking of tokens for model-agnostic XAI methods, as well grouping words together by their lemma in the post-processing of vocabulary lists which are produced by model-specific approaches.

\section{Active vs. Passive Utility}
This work has exclusively focused on the utility of words for understanding existing texts, however, one can also consider the utility of a word for active speaking and writing:
One obvious difference is that, when there are two synonyms, it suffices to know one of the two for active speaking, but for passive understanding, both must be learned.
However, testing active utility would likely be a major challenge in this approach.

\section{Multilinguality}
While we have made an effort to make our implementation handle as many languages as possible, we have only tested it on English corpora.
An extension of this work should include evaluation on the other languages supported by the data pipeline.

\section{Distinction of Homonyms}
Some words can have multiple meanings:
An example of this is the word \textit{can}, which can signify either a container (a tin can) or the modal verb (to be able to do something).
Such words are called \textit{homonyms}, and are not distinguished by our current implementation.
In order to produce more useful lists of vocabulary, distinction between the different meaning of such words could be a useful component, especially when one of the meanings is more common than the other.
Handling homonyms as separate entities could be performed using BERT word embeddings \cite{kentonBertPretrainingDeep2019}, which map words to vectors.


\section{Advanced XAI Methods}
As mentioned before, the XAI methods used in this work are fairly simple.
This is in part because we attempted to find important words from multiple points of view:
Single Token Summary finds the most useful words to learn for beginners, while Single Token Ablation evaluates words highly which are useful for advanced learners, due to the differences in how they perturb the input.
Still, more advanced XAI methods could help to improve the list generation. 



