\section{Evaluation Measures}

The various utility extraction approaches produce ranked lists as outputs.
To compare these, we employ both quantitative and qualitative comparison approaches, described in the following sections.

\subsection{NLP Task Performance}
The primary metric for evaluating a vocabulary list's efficiency will be the approach explained in Section \ref{sec:list-generation}: 
We let an AI model perform an NLP task with a vocabulary consisting of the first $k$ elements of the list.
The limited vocabulary is simulated by removing words outside the vocabulary from the inputs to the model.
We then progressively increase the vocabulary by increasing index $k$ until we reach the end of the list.
The increase of vocabulary size is exponential, as this will ensure a quick evaluation and performance is expected to not grow very much towards the end of the list, where the less useful words should be (this assumption can be confirmed in the evaluation results in Section \ref{sec:results}).

Listing \ref{alg:efficiency-evaluation} shows pseudocode for the described setup:

\todo{Edit surrounding descriptions of efficiency to be in line with pseudocode}
\input{listings/efficiency_evaluation_pseudocode.tex}

This approach gives us a direct, quantitative metric for the efficiency of a vocabulary list.




\subsection{List Similarities}
In order to compare the results provided by the various approaches, metrics are needed that can be consistently calculated across the different approaches.
While a human may be able to qualitatively analyze lists and gain a rough idea of their similarity, computed metrics provide an instantaneous (if simplified) outlook on similarities.
A metric shall be defined as a function which takes as parameters two word lists of equal length which are word lists ordered descendingly by supposed relevance, and outputs a real number giving either a distance or similarity between the lists.

Considerations of the choice of metric are:

\begin{description}
	\item [Handling of lists with partial overlap.]
	      Metrics must be able to handle elements which occur in only one of the two lists.
	      Thus, a metric which solely compares ranks of elements is not viable.
	\item [Start of lists is more impactful than end.]
	      Since the beginning of lists contains the words which are ranked as most important, changes at the top should impact the metric more than changes at the bottom. This includes (1) Equal differences in rank should be counted as more important if they occur further up the list.
	      A word that is rank 1 in list A but rank 101 in list B says more about the similarity than if a word is rank 2000 in list A but rank 2100 in list B. Likewise, if a word is absent from list B, it implies a greater difference if that word is at rank 1 in list A than if it were at rank 1000.
\end{description}

\subsubsection{Metrics Used}

\begin{description}
	\item [Sequential rank agreement (modified)] \cite{ekstromSequentialRankAgreement2015}: This metric is based on the deviations of some subset of the lists in the upper ranks.
	      It is important to note that this metric has an additional parameter "depth" which determines how many elements (from the top of the list) are considered.
	      It is therefore more helpful to view its results at various depths.
	      The original formula for this metric in the case of two lists is:
	      \[
		      a_{d} := a \text{ from start to rank } d
	      \]

	      \[
		      S_{d} := a_{d} \cup b_{d}
	      \]

	      \[
		      SRA_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \left( r_{b}(x) \right) - \left( r_{a}(x) \right) \right)}{|S_{d}|}
	      \]

	      where \(\lambda\) is a normalization factor ensuring that \(\max(SRA) = 1\).
	      In its proposed form, this metric can only compare lists which contain the same set of unique elements, just in different orders.
	      In order to make it work on lists where this is not the case, one can set the "rank" of nonexisting elements to a value greater than the length of the lists, such as \(2 |a|\).
	      Another drawback of the metric is that the standard deviation of two numbers does not depend on their absolute value, only their difference.
	      However, to satisfy number 3 of the stated requirements, we can take the deviation of the logarithm of the ranks instead of the deviation of the ranks themselves, resulting in the formula

	      \[
		      r^{\prime}(x) :=
		      \begin{cases}
			      \mathrm{rank}_{b}(x) & \text{if } x \in b, \\
			      2 \cdot |a|          & \text{otherwise.}
		      \end{cases}
	      \]

	      \[
		      SRA^{mod}_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \log (r^{\prime}_{b}(x))-\log(r^{\prime}_{a}(x))\right)}{|S_{d}|}
	      \]

	      For this modified version, \(\lambda\) can be calculated with:

	      \[
		      \lambda = \frac{1}{SRA_{d}(a, a^{*})},
	      \]

	      where \(a^{*}\) is a list such that \(a \cap a^{*} = \emptyset\).
	      % \item \textbf{Average overlap / rank-biased overlap} \cite{webberSimilarityMeasureIndefinite2010}: Compares ranked list and puts more emphasis on the top of the list than at the bottom. However, assumes that the elements of both lists are the same, i.e., that all elements of list A are also in list B and vice versa.
	      %
	      % \item \textbf{11-point interpolated average precision} \cite{manningIntroductionInformationRetrieval2008}: Uses set metric precision (though may also use recall or F1 score on various subsets of the list (first 10\%, first 20\% etc.) and takes their geometric mean to arrive at a single number.
	      %       As each of the elevent numbers is calculated on the partial subset of the list's elements starting at the first element, this means that changes in the top of the list affect more of these numbers and thus have a larger impact on the final calculated mean.
	      %       One drawback of this method is that precision measurement within the 10\% interval only takes into account set membership, not the order of words:
	      %       For a list containing 10,000 words, the evaluated intervals are words in the index intervals $\left[ 0, 1000 \right), \left[ 0, 2000 \right), ... ,\left[ 0, 10000 \right)$.  Differences of order within the first 1000 words are thus ignored.

	\item [Discounted Cumulative Gain]:
	      This formula outputs a value between 0 and 1, with 1 being given if both lists are identical, 0 when they have no elements in common, and values in between when there is partial overlap between elements and/or their order is different.
	      ${DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}=rel_{1}+\sum _{i=2}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$
\end{description}

\[
	rel_{i} :=
	\begin{cases}
		\frac{1}{rank_{b}(el_{i}) + 1} & \text{if } el_{i} \in b \\
		0                              & \text{otherwise}
	\end{cases}
\]
\subsubsection{Metrics Rejected}
\begin{description}
	\item [Kendall rank correlation ] \cite{kendallNEWMEASURERANK1938b}: This metric is bounded between 0 and 1 and compares the ranks of the elements of two lists. However, it cannot handle elements that only occur in one of the two lists, and thus is not suitable for our purposes. It also does not distinguish between differences in the upper and lower parts of the lists.
	\item [Spearman's footrule] \cite{spearmanCorrelationCalculatedFaulty1910}: Rejected for the same reasons as Kendall rank correlation.
\end{description}


\subsubsection{Tests of Applicability}
[Results of preliminary tests of various metrics on own lists such as rank switching ,replacement at bottom and top of list etc.]


\section{Baselines}
It may be useful to compare the lists generated by the various approaches with existing word lists from educational materials:
Textbooks often feature chapters with word lists, or sentences which can be converted to word lists with a tokenizer.
The purpose is to have a point of comparison, to see if generated lists agree with existing lists, and find reasons for differences.

\subsection{Existing Lists}
As one point of comparison, we can use existing vocabulary lists that are readily available in digital format, regardless of the method which created them.
For this, a survey was done on popular language learning tools which make public some of their learning data.



\todo{	Language learning textbooks}

\subsubsection{Duolingo}
While Duolingo is the most popular language learning application as of 2024, it does not publish its word lists or course contents that is free of cost and easily convertible to a format that can be processed with NLP tools.

\subsubsection{Rosetta Stone}
\Rosetta \footnote{\url{https://rosettastone.co.jp/}} is a popular online and mobile language learning platform.
\Rosetta\ publishes some of their course contents on its website \footnote{\url{https://support.rosettastone.com/s/article/Rosetta-Stone-Course-Content}}.
These lessons take the form of phrases that should help a learner to quickly gain competence in their target language.
To make a list of vocabulary from \Rosetta 's Lesson Contents, we use a tokenizer on the lesson and let the order of first appearance of a wordi n the lesson be its rank in the resulting vocabulary list.
Thus, the generated list features those words first which \Rosetta\ introduces first to its learners, which presumable is close to the word order that \Rosetta estimates is most useful.
One qualification of the previous statement is that some words are probably only introduced so the learner has something to talk about, rather than because of the word's inherent utility.



This section has introduced some pre-existing lists as points of comparison for our generated vocabulary lists.
However, one shortcoming of this comparison is that these lists have not been prepared with the same linguistic contexts in mind as those made by this work's approach, and thus their utility is naturally greater in general-context corpora than specialized ones.
While the context specificity of this work's utility extraction method is a major advance for creating vocabulary lists, it also makes this an unfair comparison.

However, apart from pre-existing lists, there also exist pre-existing \textbf{methods} of finding important words in texts, which we can also use as baselines to compare with our method.
These also use corpora, but not AI models or XAI methods to find important words.
The next section will introduce some of these methods, which we will use to generate comparison lists to our own.


\subsection{Existing List Generation Methods}
To get an impression of how well our XAI-based method for vocabulary list generation performs, it will be informative to test established methods for extracting important words from texts. 

\subsubsection{Raw Frequency}

Raw word frequency has been used as a criterion for which words to teach to language learners \cite{heChoosingWordsTeach2019}.
Frequency is much less computationally intensive than any of the XAI-based approaches, as it only requires a tokenizer to be run on a corpus, without involving model calls.

\subsubsection{Frequency, Without Stopwords}
One drawback of using raw frequency the criterion by which to order vocabulary lists is that he most frequent words in most corpora will consist of stopwords.
This can be seen in the following frequency word list from corpus [x]. 
\todo{insert sample from raw frequency word list from corpora used in experiments }

These words, while being the most generally useful, have very little meaning by themselves \cite{rajaraman2011data}.
For this reason, they are often filtered out in the inputs of NLP tasks.
Removing from the raw frequency-based word lists could be a way boost the lists's  efficiency, especially at the top of the list.
We therefore filter out known stop words, based on a list from the popular NLP library \textit{NLTK} \footnote{\url{https://www.nltk.org/}}.
\footnote{The NLTK stopword list can be downloaded by following the instructions on \url{https://www.nltk.org/howto/corpus.html\#word-lists-and-lexicons}.}


\subsubsection{TF-IDF}
\todo{either finish this section or replace with collection frequency normalization}
To characterize the contents of a document, \cite{qaiserTextMiningUse2018}.
TF-IDF has, to the best of our knowledge, not been considered for vocabulary list generation.
However, it is often employed to find key terms in a document, which characterize the document's contents with a small number of words.

TF-IDF: The frequency of a word in the context-specific corpus, normalized against a background corpus.

\subsection{LLM Prompts}
In recent years, Large Language Models have become a popular tools for language learners to find new words to learn about specific areas.
For this purpose, we run the following prompt to ChatGPT 3 for each context to get a sense of how well my method performs against this easy method.
\todo{Prompt and results}

\subsection{Sample text}
\todo{Show a sample paragraph with first n words from each list. Can be used to intuitively evaluate utilities}


\section{Results} \label{sec:results}
\contentdescription{
Discussion/Analysis [Interpretation of the results]
Could you combine approaches with each other or baselines to  combine strength, what are the strengths/weakness of each approach
}

\section{Discussion}

