Evaluation of the vocabulary list generation approaches is challenging:
We can measure the quality of each approach by measuring the efficiency of the vocabulary lists it produces.
We have described in detail our method for vocabulary efficiency evaluation in Section \ref{sec:experimental-setup-with-ai}, which involves running an AI model on a context-specific corpus with a given vocabulary, where the vocabulary is given by taking the first $k$ words from the vocabulary list in question.
However, this approach is very similar to our approach for list generation, which involves analyzing AI interaction of AI models with corpora through the use of XAI methods.
Therefore, it is natural to assume that our evaluation approach is biased towards the lists stemming from our list generation approach.
This concern is especially pertinent as it has been shown that AI models do not reliably respond comparably to humans when the inputs to NLP tasks are perturbed \cite{tjuatjaLLMsExhibitHumanlike2024}.
However, to the best of our knowledge, there has not yet been a proposed evaluation metric for vocabulary lists that can be done automatically and which outputs a simple numeric result.

To address this concern, in addition to our own evaluation approach, we also present sample vocabulary lists, use our evaluation approach, and check whether the results align with human intuition.

For this reason, our evaluation is not only concerned with analyzing statistics of measured efficiency, but also with whether the calculated efficiency values are reliable.
The questions examined in this chapter can be summed up as:
\begin{enumerate}
	\item Does our efficiency evaluation of our list correspond with human intuition? (see Section \ref{sec:does-eval-correpond-to-intuition})
	\item According to this efficiency evaluation, do the lists generated by our approach outperform lists from frequency-based approaches? (Sections \ref{sec:results-small-context} and \ref{sec:results-large-context})
	\item Taking all data (list performance, similarity, runtime, etc.) into account, which approach is suited best for which situations? Can we combine the approaches to achieve even better results? (Section \ref{sec:results-discussion})
	      % \item How can the various approaches be combined to supplement each other's weaknesses?
\end{enumerate}

The following sections first introduce several baselines against which we compare our list generation approaches.
These baselines include both publicly available vocabulary lists from online language learning tools, and existing list compilation methods introduced in Section \ref{sec:frequency-based-list-generation-methods} in connection with the corpora used for our own lists for a more direct comparison.

We then describe the (numeric) metrics used in the evaluation in Section \ref{sec:evaluation-measures}.
Finally, we present the results of the evaluation in Section \ref{sec:results}, and discuss their implications for the usability of each approach in Section \ref{sec:results-discussion}.

\section{Baselines}
It is useful to compare the lists generated by our AI-driven vocabulary list generation approaches with both existing word lists from educational materials, and frequency-based compilation approaches which use the same data as our approaches as input.
They serve as a point of comparison, to see where generated lists agree with existing lists, and to compare the efficiencies of the lists with ours.
For this reason, Section \ref{sec:exising-lists} introduces sources of external lists, and Section \ref{sec:freq-generation-methods-baselines} reviews the frequency-based approaches used as baselines.

\subsection{Existing Vocabulary Lists} \label{sec:exising-lists}
As one point of comparison, we can use existing vocabulary lists that are readily available in digital format, regardless of the method which created them.
For this, a survey was performed on popular language learning tools which make public some of their learning data.
However, as most tools do not publish their vocabulary lists free of cost in text form, we only obtained one list for comparison from the online learning platform \Rosetta\, as the following subsections show.

\subsubsection{Rosetta Stone}
\Rosetta \footnote{\url{https://rosettastone.co.jp/}} is a popular online and mobile language learning platform.
\Rosetta\ publishes some of the course contents on its website \footnote{\url{https://support.rosettastone.com/s/article/Rosetta-Stone-Course-Contents}}.
These lessons take the form of phrases intended to help learners to quickly gain competence in their target language.
To make a list of vocabulary from \Rosetta 's lesson contents, we use the freely available lessons  of the \textit{American English} course (Units 1-20).
We convert the PDF files to text format, filter lines which do not contain lessons contents (e.g., "Lesson 8" and similar ancillary notes).
On the filtered text, we run the \texttt{bert-base-uncased} tokenizer, merge the sub-word tokens to words and let the order of first appearance of a word in the lessons be its rank in the resulting vocabulary list.
Thus, the generated list features the lessons' words in the order in which \Rosetta\ introduces them to its learners, which presumably is close to the word order that \Rosetta\ estimates to be most useful.
One qualification of this statement is that some words may be introduced in order for the learner to have something concrete to talk about, rather than because of their estimated utility.
The final vocabulary list contains 2,545 words in total.

\subsubsection{Unsuitable Sources of Vocabulary Lists}
The following sources of vocabulary lists were considered, but not used for the reasons described below.

\paragraph{Textbooks:}
An obvious source of vocabulary lists to compare our lists against are English textbooks, which often contain vocabulary lists for each lesson.
However, the books we considered as candidates, such as \textit{Practical English Usage} by Michael Swan \footnote{Available at Oxford University Press at \url{https://elt.oup.com/catalogue/items/global/grammar_vocabulary/practical_english_usage_4th_edition/9780194202510} (last accessed on April 25, 2025)}, do not publish their course contents free of charge.

\paragraph{Cambridge Word Lists:}
Cambridge publishes word lists for certain levels of English \footnote{\url{https://www.cambridgeenglish.org/images/149681-yle-flyers-word-list.pdf}}.
However, these lists are meant for assessing learner's level of vocabulary skills, and the list for each level is ordered by alphabetical order, rather than the perceived level of the word, making them unusable as comparison to our approach.

\paragraph{Duolingo:}
While \textit{Duolingo} \footnote{\url{https://duolingo.com/}} is a popular language platform, it does not publish its course contents in a format that is easily convertible to text and thus be converted to lists of vocabulary.

% This section has introduced some pre-existing lists as points of comparison for our generated vocabulary lists.
% However, one shortcoming of this comparison is that these lists have not been prepared with the same linguistic contexts in mind as those made by this work's approach, and thus their utility is naturally greater in general-context corpora than specialized ones.
% While the context specificity of this work's utility extraction method is a major advance for creating vocabulary lists, it also makes this an unfair comparison.
%
% However, apart from pre-existing lists, there also exist pre-existing \textbf{methods} of finding important words in texts, which we can also use as baselines to compare with our method.
% These also use corpora, but not AI models or XAI methods to find important words.
% The next section introduces some of these methods, which we use to generate comparison lists to our own.


\subsection{Frequency-Based List Generation Methods} \label{sec:freq-generation-methods-baselines}
To gain an impression of how well our XAI-based method for vocabulary list generation performs, we employ the three frequency-based methods for generating lists of vocabulary introduced in Section \ref{sec:frequency-based-list-generation-methods}, namely Raw Frequency, Average Reduced Frequency, and TF-IDF.
We use these three measures as baselines to our methods, again using the tokenizer \texttt{bert-base-uncased} (and merging tokens into words) to divide texts into words that we can count.
To calculate the inverse document frequency in TF-IDF, we use a random sample of 53,178 documents from the Oscar corpus (see Section \ref{sec:oscar}).

One can view these frequency approaches as assigning corpus utility to words in the same way that our AI-driven approaches do, with the corpus utility of a word being given by its frequency/average reduced frequency or TF-IDF.
Therefore, for the large context scenario, their utility aggregation is also performed by summing over the corpus utilities for the single corpora in the large context, as explained in Section \ref{sec:impl-summary-description}.

In this section, we have introduced our baselines, namely the \Rosetta\ vocabulary list, and the three frequency-based list generation methods.
The next section will discuss how we evaluate these baselines, as well as our own various approaches and lists.

\section{Evaluation Measures}\label{sec:evaluation-measures}
The various list generation extraction approaches produce ranked lists as outputs.
To compare these, we employ our evaluation approach as described theoretically in Section \ref{sec:experimental-setup-with-ai}.
Section \ref{sec:task-performance-measure} discusses the implementation of this approach.
In addition, we compare the lists by how similar they are to each other in Section \ref{sec:list-similarity-measure}.

\subsection{NLP Task Performance} \label{sec:task-performance-measure}
Our primary metric for evaluating a vocabulary list's efficiency is the approach explained in Section \ref{sec:experimental-setup-with-ai}:
We let an AI model perform an NLP task on a single corpus, using a vocabulary $V_{l, k}$ which consists of the first $k$ elements of the list $l$.
The limited vocabulary is simulated by removing words outside the vocabulary from the inputs to the model.
For illustration, consider again the example sentence:

\begin{quote}
	\textit{Abraham Lincoln faced enmity in 1863.}
\end{quote}

If our vocabulary consists only of the word \textit{enmity}, this sentence becomes:

\begin{quote}
	\textit{Abraham Lincoln enmity 1863.}
\end{quote}

Since \textit{Abraham Lincoln} is recognized as a named entity, we leave it in the input, even though it is not part of the vocabulary.
Non-alphabetic tokens, such as numbers and punctuation, are also left in the input.

Listing \ref{alg:efficiency-evaluation} shows pseudocode for our implementation of the list evaluation approach:

\input{listings/efficiency_evaluation_pseudocode.tex}

We start the evaluation of every vocabulary list with a vocabulary size of 0, letting the model run on inputs where only named entities and non-alphabetic tokens are left.
The score for the entire corpus is calculated by taking the arithmetic mean of the scores of individual lines.
These runs with a vocabulary size of 0 lets use observe the "utility" of the named entities, putting into context the performance scores when vocabulary is added.
After this, we set the vocabulary size to 10 words, and run the AI models with the resulting vocabulary $V_{l, 10}$.
We then progressively increase the vocabulary by increasing index $k$ by a factor of 4, until we reach the end of the list.
The increase of vocabulary size is exponential, as this ensures a quick evaluation and performance mostly increases at the start of the list, not grow very much towards its end (this is confirmed by the evaluation results shown in Sections \ref{sec:results-small-context} and \ref{sec:results-large-context}).

As stated in Section \ref{sec:sentence-embedding}, we employ the sentence embedding model in the evaluation.
We measure the performance of the model for a sentence variation (with only the words in the vocabulary) by how close the output vector is (in cosine distance) to the output vector of the unaltered sentence.
In order to let the score take on values between 0 and 1, we normalize the line score by also taking the distance between the baseline vector and the model output for an empty string (\texttt{""}).

This approach gives us a direct, quantitative metric for the efficiency of a vocabulary list at each of the test vocabulary sizes.
As an illustration, we present the results of one such efficiency evaluation run for three vocabulary lists on the Wikipedia Article of American actor Marlon Brando \footnote{\url{https://en.wikipedia.org/wiki/Marlon_Brando}} in Figure \ref{fig:marlon-brando-eval}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{marlon-brando-eval.png}
	\caption{Efficiency evaluation results on vocabulary lists for the Wikipedia Article of Marlon Brando.}
	\label{fig:marlon-brando-eval}
\end{figure}


To make the figure readable, only three list generation approaches are compared, namely Raw Frequency, TF-IDF, and Single Token Ablation with the sentence embedding model.
We first evaluate the AI model performing the task with a vocabulary size of zero, simulating the state of not knowing any words.
The only tokens in this run are named entities and punctuation, as these are not considered to be potential vocabulary terms.
The vocabularies are then evaluated with an initial size of 10, and increase by a factor of 4 in each step, until a final evaluation is done with the full vocabulary list.
For single corpora, the full list contains all words from the corpus, which is why the performance reaches 1 with all lists.

To compare the results on a broader scale, we aggregate the results of the evaluation runs for each list generation evaluation method as listed in Section \ref{sec:impl-summary-description}.
For both the small context and the large context scenario, we take the average across the lists' scores across the single corpora.
The difference in the scenarios in the evaluation is that for the small context, a list is individually compiled from and evaluated on each single corpus in the large context, whereas for the large scenario, the same vocabulary list is evaluated on all single corpora in the evaluation set of the context (see Section \ref{sec:final-dataset}).

This section has described our approach to evaluating the efficiency of vocabulary lists.
However, as this approach is a novel one, we also compare the \textit{similarity} of the vocabulary lists as a sanity check.
The next section will introduce the metrics we use for this comparison.

\subsection{List Similarity} \label{sec:list-similarity-measure}
Apart from our own vocabulary list evaluation approach that measures the performance of an AI model using the vocabulary list, we also compare how similar the lists are in general.
This allows us to check whether the results of our efficiency evaluation are plausible.
It also serves another purpose, for the case when two approaches for list generation produce vocabulary lists of similar efficiencies, but one takes much less time:
If the lists of both approaches are not only similar in efficiency, but also in content, we can safely say that the approach taking less time is objectively the superior one (for the data observed).

In order to compare the similarity of the resulting vocabulary lists provided by our approaches, metrics are needed that can be consistently calculated across the different approaches.
While a human may be able to qualitatively analyze lists and gain a rough idea of their similarity, computed metrics provide an instantaneous (if simplified) outlook on similarities.
By a metric, we mean a function which takes as parameters two ranked word lists of equal length and outputs a real number giving either a distance or similarity between the lists.

Two considerations influenced our choice of the similarity metrics we employed in our evaluation:
Firstly, the metrics must be \textbf{able to handle elements which occur in only one of the two lists}.
Secondly, differences between elements at \textbf{the tops of the lists must impact the metric more} than differences between elements at the ends.
This is because the top of each vocabulary list contains the words which are ranked as most important.

While Kendall rank correlation \cite{kendallNewMeasureRank1938c} and Spearman's footrule  \cite{spearmanCorrelationCalculatedFaulty1910} are simple metrics to compare ranked lists, they make no distinction between differences at the tops of lists and differences at their ends, in addition to being difficult to adapt to handle for missing elements.
Instead, we use two metrics called Average Overlap and Normalized Discounted Cumulative Gain, which we introduce in the following subsections.
%
% \subsubsection{Sequential rank agreement (modified)}
% Sequential rank agreement \cite{ekstromSequentialRankAgreement2015} is based on the deviations of some subset of the lists in the upper ranks.
% 	      It is important to note that this metric has an additional parameter "depth" which determines how many elements (from the top of the list) are considered.
% 	      It is therefore more helpful to view its results at various depths.
% 	      The original formula for this metric in the case of two lists is:
% 	      \[
% 		      a_{d} := a \text{ from start to rank } d
% 	      \]
%
% 	      \[
% 		      S_{d} := a_{d} \cup b_{d}
% 	      \]
%
% 	      \[
% 		      SRA_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \left( r_{b}(x) \right) - \left( r_{a}(x) \right) \right)}{|S_{d}|}
% 	      \]
%
% 	      where \(\lambda\) is a normalization factor ensuring that \(\max(SRA) = 1\).
% 	      In its proposed form, this metric can only compare lists which contain the same set of unique elements, just in different orders.
% 	      In order to make it work on lists where this is not the case, one can set the "rank" of nonexisting elements to a value greater than the length of the lists, such as \(2 |a|\).
% 	      Another drawback of the metric is that the standard deviation of two numbers does not depend on their absolute value, only their difference.
% 	      However, to satisfy number 3 of the stated requirements, we can take the deviation of the logarithm of the ranks instead of the deviation of the ranks themselves, resulting in the formula
%
% 	      \[
% 		      r^{\prime}(x) :=
% 		      \begin{cases}
% 			      \mathrm{rank}_{b}(x) & \text{if } x \in b, \\
% 			      2 \cdot |a|          & \text{otherwise.}
% 		      \end{cases}
% 	      \]
%
% 	      \[
% 		      SRA^{mod}_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \log (r^{\prime}_{b}(x))-\log(r^{\prime}_{a}(x))\right)}{|S_{d}|}
% 	      \]
%
% 	      For this modified version, \(\lambda\) can be calculated with:
%
% 	      \[
% 		      \lambda = \frac{1}{SRA_{d}(a, a^{*})},
% 	      \]
%
% 	      where \(a^{*}\) is a list such that \(a \cap a^{*} = \emptyset\).

\subsubsection{Average Overlap}
The Average Overlap \cite{webberSimilarityMeasureIndefinite2010} of two lists is given by the average cardinality of the intersection (overlap) of the sets given by taking the two lists up until every possible index.
Because the first element contributes to every iteration, it causes the tops of the lists to impact the metrics more than their ends.
There is also no issue with partial overlaps between the lists.
Using the notation of a vocabulary $V_{l, k}$ as a set of words given by vocabulary list $l$ until an index $k$ (see Section \ref{sec:voc-list-efficiency}), the average overlap between two vocabulary lists like is given by:

\begin{equation}
	\text{AverageOverlap}(A, B) = \frac{1}{n} \sum_{k=1}^{n} \frac{|V_{A, k}  \cap V_{B,k}|}{k}
\end{equation}

% \item \textbf{11-point interpolated average precision} \cite{manningIntroductionInformationRetrieval2008}: Uses set metric precision (though may also use recall or F1 score on various subsets of the list (first 10\%, first 20\% etc.) and takes their geometric mean to arrive at a single number.
%       As each of the elevent numbers is calculated on the partial subset of the list's elements starting at the first element, this means that changes in the top of the list affect more of these numbers and thus have a larger impact on the final calculated mean.
%       One drawback of this method is that precision measurement within the 10\% interval only takes into account set membership, not the order of words:
%       For a list containing 10,000 words, the evaluated intervals are words in the index intervals $\left[ 0, 1000 \right), \left[ 0, 2000 \right), ... ,\left[ 0, 10000 \right)$.  Differences of order within the first 1000 words are thus ignored.

\subsubsection{Normalized Discounted Cumulative Gain (NCDG)}
(Normalized) Discounted Cumulative Gain \cite{jurafskySpeechLanguageProcessing2025} is a metric used in information retrieval to compare relevance scoring of two sets of elements.
By making the relevance score $R$ of a set element determined by its rank in our ranked lists, we adapt this metric to compare the similarity of two lists:

% This formula outputs a value between 0 and 1, with 1 being given if both lists are identical, 0 when they have no elements in common, and values in between when there is partial overlap between elements and/or their order is different.

% ${DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}=rel_{1}+\sum _{i=2}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$



\[
	\text{DCG}(A, B) = \sum_{k=1}^{n} \frac{2^{R(A_k)} - 1}{\log_2(1 + k)}
\]

The relevance $R_B$ is a function taking as input an element (not an index) of list A.
We define it as follows:

\[
	R_B(m) :=
	\begin{cases}
		\frac{1}{rank_{B}(m) + 1} & \text{if } B \text{ contains } m \\
		0                         & \text{otherwise}
	\end{cases}
\]
With relevance defined in this way, we ensure both that differences between the tops of the lists impact the measurement more than differences at the end, and the handling of elements which only appear in one of the two lists.
To normalize this metric to output values between 0 and 1, we divide by the maximum value for the list length, which is given by the DCG of one of the lists with itself:

\[
	\text{NDCG}(A,B) = \frac{1}{\text{DCG}(A, A)} \text{DCG}(A, B)
\]

Having established our metrics to evaluate both the performance of and similarity between vocabulary lists, we present the results of the evaluation in the following section.

\section{Results} \label{sec:results}
This section presents the results both for the small context and large context scenarios.
The list generation approaches are evaluated by the performance of their lists on NLP tasks, by the similarity between the lists, and by the time required to generate them.

We first establish that our approach to evaluating list efficiency does roughly align with human intuition in Section \ref{sec:does-eval-correpond-to-intuition}.
After this, we show the results for approaches in the small context scenario where lists are compiled and tested on single Wikipedia articles and subtitles in Section \ref{sec:results-small-context}, where we find that our AI-driven approaches generally outperform the baselines.
Finally, the results for the large contexts (using separate single corpora for generation and evaluation) are presented in Section \ref{sec:results-large-context}.
For the large contexts, we find that most list generation approaches produce lists of similar efficiencies, which seems to imply that frequency is a valid proxy metric for large vocabulary sizes, as it requires less time for vocabulary list generation. 

\subsection{Correspondence of Efficiency Evaluation with Human Intuition} \label{sec:does-eval-correpond-to-intuition}
As mentioned in the introduction to this chapter, we use the performance of AI models on corpora where we simulate a limited vocabulary in order to measure the efficiency of a vocabulary list.
Before we present the results of these tests, however, we examine whether the performance of AI models is a reliable indicator that the list is useful to humans as well.
This section therefore examines several vocabulary lists tested on a short sample text containing ten lines of dialog from the subtitles of the 1972 movie \textit{The Godfather}, seen here:

\input{data/sample_text_original.tex}

The following shows the text leaving only the named entities and punctuation tokens, which constitutes our starting point for the evaluation.
An evaluation score for each line is displayed on the left:

\input{data/sample_text_empty.tex}

Due to the uncased tokenizer used, all words in the reconstructed text are lowercase.
We can see that the lines containing named entities (Line 4 and 9) already have a rather high score in the evaluation, reflecting the importance of the names to the meaning of the sentences.
The other lines receive a score of 0.0, resulting in an overall score of 0.17.
We also observe that the name \textit{Moe} in line 1 is not recognized by the Named Entity Recognition model, (falsely) making it a possible vocabulary word.

To compare their intuitive utilities, we use four vocabulary lists on the corpus and see how well their first words perform when evaluated on the sample text:
\begin{enumerate}
	\item A randomly ordered list from words in the sample text.
	\item A list compiled with raw frequency.
	\item A list compiled with TF-IDF (with the Oscar corpus as normalization).
	\item A list compiled by Single Token Summary with sentence embedding.
\end{enumerate}

The tops of the lists can be seen in Table \ref{tbl:first-k-words-sample}.

\begin{table}[H]
	\centering
	\input{data/first k words sample.tex}
	\caption{Top 10 words of each compared list on the sample text.}
	\label{tbl:first-k-words-sample}
\end{table}

It can be seen that the ten top words of the lists collected with TF-IDF and the Single Token Summary approach give a better idea of the meaning of the text than the words from the random and raw frequency lists.
Next, we examine the words in the context of the sample text.
In the following listings, we see the sample text with the 10 top words filled in from each vocabulary list, in addition to the recognized named entities.
Each line displays the evaluation score for it, and the total score (calculated by taking the average across the lines) is seen in the caption of each listing:

\input{data/sample_texts.tex}

The main observation is that the texts with the TF-IDF and Single Token Summary, whose top words make the meaning of the text much closer to the meaning of the original text than those of the other two approaches, are indeed evaluated significantly higher (0.52 and 0.56 vs. 0.33 and 0.30).
In our opinion, this is evidence that our evaluation scores roughly coincide with how much the words let a reader understand the text, aligning with utility as defined in Section \ref{sec:utility}.
Both of these lists feature the essential words \textit{casino}, \textit{godfather} and \textit{gamblers}, as well as other words which let a reader guess at the situation.

We can, however, also notice some peculiarities in the evaluation scores:
The text with words from the randomly assembled list (Listing \ref{lst:sample-text-random}) seems more understandable to us than when words from the  frequency list are filled in (Listing \ref{lst:sample-text-frequency}), since the randomly compiled list contains useful words such as \textit{selling}, \textit{casino} and \textit{contract}.
However, the text with the frequency words receives a higher evaluation score than the one with the random words (0.33 vs. 0.3).
This is due partially to the unrecognized entity \textit{moe}, which is contained in the frequency list but not the random one, "artificially" boosting the score of the frequency list.
But even disregarding that name, the scores are not as disparate as we expected them to be:
For example, line 3 in the random words text \texttt{"i' make       ."} receives a (rounded) score of 0.0, while the same line in the frequency text \texttt{"i' ll      he      ."} is assigned a score of 0.3.
Since both versions of the line seem to contain very little information, this difference in the evaluation score suggests that the cosine distances between the sentence embeddings do not completely correlate with differences in meaning of the sentences, and thus the evaluation scores may not reflect the true utility of words in all circumstances.
With these observations in mind, we present the evaluation results in the next sections.

\subsection{Small Context} \label{sec:results-small-context}
This section shows the evaluation results of the vocabulary lists for the small context scenario, where each list is generated from a single corpus (single article or subtitles) and then evaluated on that same corpus.
We evaluate the efficiencies of the lists, their similarities, and the time needed to compile them.
Each of the approaches from \ref{sec:impl-summary-description} is evaluated, as well as the three frequency-based approaches.
We do not include the \Rosetta\ list in the comparison, as it is too generic to be compared against lists specifically compiled for a short context.
Instead, the \Rosetta\ list is used in the next section, showing the results for the large context scenario.

% Table \ref{tbl:performance-results-single-opensubs} and \ref{tbl:performance-results-single-wikipedia} show the aggregated results of single corpora, separated by corpus type.

\subsubsection{List Efficiency}
This section shows and discusses the efficiencies of the lists generated by each generation approach , as evaluated with the sentence embedding model (see Section \ref{sec:task-performance-measure}).
The following tables show the results , at the test vocabulary sizes, with warm colors indicating higher efficiency.
The highest score at each vocabulary size is highlighted in \textbf{bold} font.
Performance scores were rounded to two decimals to improve readability.

\input{data/performances small all.tex}

We can observe several points in the data:
Firstly, an obvious observation is that the \textbf{lists generated with the sentence embedding model outperform the lists generated by the next sentence prediction model.}
Because sentence embedding is used for the evaluation, this is a foreseeable result.
Another point to observe is that the lists generated with the NSP model generally show equal or lower scores than even the raw frequency baseline.
This suggests that the \textbf{utility of words depends strongly on the NLP tasks} performed in the tests.

Next, we observe patterns in the XAI methods used:
With the sentence embedding model, \textbf{Single Token Summary and Single Token Ablation achieve the highest results across contexts}, producing notably higher scores than any of the frequency-based methods.
More specifically, Single Token Summary is best at small vocabulary sizes, which conforms to our intention stated in \ref{sec:single-token-summary} to capture the most useful words for beginners with Single Token Summary.
Single Token Ablation shows better efficiency for large vocabulary sizes, which makes sense as ablating a single token essentially simulates the viewpoint of an advanced language learner who knows all the words in the input sentences except one, and finds the one that maximizes the learner's performance.

The \textbf{Progressive Summary} approach, despite being a more complex approach, does not outperform Single Token Ablation or Single Token Summary.
The comparison is difficult, as the aggregation of line utilities for Progressive Summary is not as straightforward as for Single Token Ablation (see Section \ref{sec:progressive-summary}).
For this reason, it seems that investigating possible improvements for how line utilities can be aggregated to calculate corpus utility may not be productive.

The \textbf{transformer attention} XAI approach achieves results that are superior to the frequency baselines at high vocabulary sizes, but its scores are close to them for lower sizes, and it is even outperformed by TF-IDF in that range.
\textbf{TF-IDF}, despite its simplicity, produces lists of an efficiency very close to the XAI methods, especially at low vocabulary sizes.
TF-IDF, while usually used to find words that capture the topic of documents, also seems to capture useful words rather well.
This could be seen as evidence to the effect that, while learning words by their general frequency provides the best coverage of texts, the coverage is not the best good proxy for measuring the understanding of a (relatively short) text.
We can thus see that normalizing the raw frequency of words against a generic background corpus, such as an \textit{Oscar} corpus, improves the efficiency of vocabulary lists.

Observing the zero-vocabulary scores, we also observe that the type of corpus has a significant impact on the performance.
The average zero-vocabulary performance was 0.17 for subtitles, 0.46 for the Wikipedia category \textit{1980s English-language films} and about 0.53 for the other categories, respectively.
In these numbers, we see the impact of the (recognized) named entities which are left in the inputs for zero-vocabulary tests:
Named entities are least relevant for the understanding of subtitles, and most relevant for people-related Wikipedia articles, with articles about movies falling in between these two extremes.

This section has analyzed the performance of the lists from the list generation approaches on the sentence embedding task.
We find that Single Token Ablation and Single Token Summary produce the best results, while attention, Progressive Summary and TF-IDF still mostly outperform the raw and average reduced frequency baselines.
The next section analyzes the similarity of the lists, to check if these results seem plausible.

\subsubsection{List Similarity} \label{sec:small-context-similarity}
This section analyzes how similar the lists generated by the approaches are to each other for the small context scenario, measured by their Average Overlap and Normalized Discounted Cumulative Gain as described in Section \ref{sec:list-similarity-measure}.
As these results mostly serve to check if the performance results of the lists are plausible, and because two metrics are used, we do not group the results by the larger context (subtitle or Wikipedia category) the list belongs to.

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_single average-overlap.tex}
	}
	\caption{Average Overlap similarity of small context vocabulary lists.}
	\label{tbl:similarity-results-single-average-overlap}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_single ndcg.tex}
	}
	\caption{NDCG similarity of small context vocabulary lists.}
	\label{tbl:similarity-results-single-ndcg}
\end{table}

First, we observe that while the scales of the two metrics are different, they generally agree on relative similarity between approaches:
If the average overlap of two approaches is greater than two other approaches, the NDCG of the former is generally greater too.
Next, we see that the two highest-performing approaches, namely \textbf{Single Token Ablation and Single Token Summary, have among the highest list similarities to each other.}

A surprising finding is that the \textbf{lists generated with the next sentence prediction model have low similarity with the lists of almost all other approaches.}
This could be an indication that either the NSP task is influenced by different words than the sentence embedding task, or it could point to issues with our word utility evaluation with the NSP model.

Another point to note is that TF-IDF and Single Token Summary show relatively high similarity, which is plausible, as both metrics tend to prioritize a small set of words characterizing the entire text, shown by their high performances at low vocabulary sizes in the previous section.
The lists generated by attention are also very similar to the lists generated by raw frequency, which explains why their scores are similar for small vocabulary sizes.
For larger sizes, attention lists produce better scores, but as our similarity metrics weigh the tops of the lists more than their bottoms, this discrepancy is not seen in the similarity scores.

In general, the similarity scores \textbf{mostly align} with our previous expectations, except that they show the lists generated with using the NSP model to be very different from the other lists, casting doubt on our implementation of these list generation approaches.
The next and final section for the small context evaluation scenario presents the differences in the time taken to generate the vocabulary lists.

\subsubsection{Speed} \label{sec:eval-speed}
When evaluating the broad applicability of a vocabulary list generation approach, the efficiency of generated vocabulary lists is not the only criterion:
The runtime of the approach is also an important factor.
Table \ref{tbl:runtime-results} shows the average runtime for the approaches per corpus type.
It must be kept in mind that the runtime is not only influenced by the word utility evaluation method, but also by implementation details, such as whether batch processing of inputs by the AI models is exploited, and the size of the model employed.
Named Entity Recognition is performed within each method, and contributes most of the runtime for the frequency-based approaches.

\begin{table}[ht]
	\centering
	\input{data/runtimes.tex}
	\caption{Runtime of list generation approaches.}
	\label{tbl:runtime-results}
\end{table}

As is to be expected, the frequency-based approaches are much faster than the XAI-based approaches.
Among the XAI approaches, we can make several observations:
First, there is a marked difference in runtime between the NSP model and the sentence embedding model.
This seems to be due both to the greater complexity of the NSP model (having 12 attention heads instead of the 6 of the sentence embedding model), and the fact that it processes more data in one call (a pair of lines instead of a single line).
Attention is the fastest list generation approach, owing to the fact that only one model call is necessary per line explanation.
Attention could be made even faster with batch processing, but since it is a fairly efficient method already, we did not optimize it to the fullest extent.

On average, most of the XAI approaches take more time on subtitles than one Wikipedia articles, with the exceptions of Progressive Summary.
This aligns with our understanding of the characteristics of the corpora and the approaches:
Wikipedia Articles have fewer lines (see Table \ref{tbl:corpus-sizes}), but more words per line, making the Progressive Summary approach take longer due to its time complexity of $\mathcal{O}(n^2)$ with respect to the number of words in the input sentence.

While neither Single Token Ablation and Single Token Summary require very much time per list, the lower runtimes of TF-IDF and attention are an argument to use these approaches when time is an important factor, as their lists' performance is only slightly below that of the two best XAI approaches.

Having completed the evaluation for the small context scenario, we discuss the results of the list generation approaches applied to large contexts in the next section.

\subsection{Large Context} \label{sec:results-large-context}
So far, we have analyzed the efficiencies of vocabulary lists with respect to the same corpus which was used to generate them in the first place.
This section discusses the efficiency and similarity of lists in the large context scenario, where lists are generated from a larger set of single corpora in a context, and evaluated on a disjoint set from the same context (see Section \ref{sec:final-dataset}).
As each approach in this scenario only creates one list per context, we first show part of the lists themselves to gain an idea what kind of lists each approach generates, before again analyzing the lists' performance and similarity.
Runtimes are not analyzed, as the list generation for larger contexts is simply the process of arithmetically aggregating the corpus utilities for the single corpora within it, which were cached values in our implementation.

\subsubsection{Top Words of Each List}
To first gain an overview of the resulting lists, we present the first 20 words of each vocabulary list generated from the sample of 632 subtitles:

\begin{table}[H]
	\centering
	% \resizebox{\textwidth}{!}{%
	\input{data/first k words OPENSUBS.tex}
	% }
	\caption{Top 20 words of the generated lists for the OpenSubtitles dataset.}
	\label{tbl:first-k-words-opensubs}
\end{table}

We can see some similarities across all lists:
Every list is headed by the two pronouns \textit{i} and \textit{you}, which seems reasonable for movie dialog.
After these, we can see where the approaches differ:
Raw Frequency and Average Reduced Frequency produce very similar lists, consisting mostly of stop words at the top.
The list made from using attention as explanation follows a similar pattern.
In contrast, the lists generated from both TF-IDF and feature importance explanations show words that are more useful for a beginner language learner in our view:
All of them contain at least one verb (\textit{know}), as well as several words that can constitute a meaningful sentence by themselves, such as \textit{what}, \textit{okay}, \textit{right}, \textit{yeah}.
Next, we present the results of the efficiency evaluation of the large lists produced on the (separate) test datasets.
We include the vocabulary list produced from the \Rosetta\ lesson in this evaluation, since both it and our own lists were not specifically compiled for the evaluation data (though our lists were compiled on more similar data).


% \begin{table}[H]
% 	\centering
% 	\resizebox{\textwidth}{!}{%
% 		\input{data/performance opensubs-subtitle big.tex}
% 	}
% 	\caption{Model performance across vocabulary sizes on subtitles, with separate train/test subtitles.}
% 	\label{tbl:performance-results-opensubs-large}
% \end{table}
%
%
% \begin{table}[H]
% 	\centering
% 	\resizebox{\textwidth}{!}{%
% 		\input{data/performance wikipedia-article big.tex}
% 	}
% 	\caption{Model performance across vocabulary sizes on Wikipedia articles, with separate train/test articles.}
% 	\label{tbl:performance-results-wikipedia-large}
% \end{table}


\subsubsection{List Efficiency}
\input{data/performances big all.tex}

We can see that the results for most approaches achieve \textbf{very similar} evaluation scores, with difference of at most 0.03 per tested vocabulary size.
The exceptions are the \Rosetta\ list and the lists generated with the NSP model, which perform significantly worse than either the frequency or the other XAI lists.

The (mostly) small gap in the list efficiencies is slightly surprising, as the subjective evaluation seen above would seem to imply that the lists compiled with feature attribution methods should outperform the frequency-based metrics.
It can be seen that the \textbf{Single Token Summary approach still outperforms the other approaches for small vocabulary sizes.}
\textbf{For larger vocabulary sizes, raw frequency and Single Token Ablation produce the highest scores.}

TF-IDF also produces very good results for small vocabulary sizes, but is outperformed by raw frequency at large sizes.
Herein, we can see a pattern that metrics designed to find very characteristic words, such as TF-IDF and Single Token Summary, are excellent for finding small sets of words which already contain much of the meaning of the texts they are used on, but fail to find the most meaningful words for larger vocabulary sizes.
This could be because these metrics do not take into account the words around the most important ones, making them fail to extract words which are useful in combination with each other.
% Another reason for their lower performance in comparison with raw frequency at large vocabulary sizes could be that, for separate generation and evaluation texts, finding words that are very specifically useful for the generation datasets brings little benefit for the evaluation corpora.

We take the high performance of the raw frequency baseline, as well as the generally small differences in these result as an indication that, \textbf{for large contexts, frequency-based approaches are a viable way to collect vocabulary}, with more complicated methods only bringing marginal improvements to vocabulary list efficiency.
In the next section, we briefly discuss the similarities between the generated lists for large contexts.

\subsubsection{List Similarity}
The following tables show the similarities between lists from the various generation methods, against with Average Overlap and NDCG:

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_big average-overlap.tex}
	}
	\caption{Average Overlap similarity of large context vocabulary lists.}
	\label{tbl:similarity-results-big-average-overlap}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_big ndcg.tex}
	}
	\caption{NDCG similarity of large context vocabulary lists.}
	\label{tbl:similarity-results-big-ndcg}
\end{table}


Because vocabulary lists for large contexts are much longer, we see higher similarity values between the lists. 
Another plausible reason for the higher similarity is that, because they are generated across much more data, they are more stable and more similar to each other than the lists for single corpora we examined in Section \ref{sec:small-context-similarity}.

Look at individual approach pairs, a striking observation is that the list generated from attention are very similar to those of raw frequency.
This implies that the importance attributions given by attention are either fairly even within lines (as raw frequency can be seen as a feature attribution methods which simply attributes equal importances to all tokens in the input), or that uneven attentions across single corpora cancel each other out, leading to even total distributions.

We once again observe that the lists generated with the next sentence prediction model are not similar to the other lists, strengthening our doubts about this method or the implementation of it.
We see a high similarity between the TF-IDF and the Single Token Summary lists, which, together with their similar performance on the large context evaluation scenario and the much lower runtime of TF-IDF, is an \textbf{argument for the use of TF-IDF for useful large contexts lists}, where the generation with XAI methods can be time-consuming.
In the next and final section of our evaluation, we summarize and discuss the findings we saw in the results.


\section{Discussion} \label{sec:results-discussion}
At the start of this chapter, we formulated three questions:


\begin{enumerate}
	\item Does our efficiency evaluation of our list correspond with human intuition?
	\item According to this efficiency evaluation, do the lists generated by our approach outperform lists from frequency-based approaches?
	\item Taking all data (list performance, similarity, runtime, etc.) into account, which approach is suited best for which situations?
\end{enumerate}

To the first question, the example texts presented in Section \ref{sec:does-eval-correpond-to-intuition} show in our opinion that the evaluation results broadly align with (our) human intuition, i.e., that they evaluate vocabularies containing subjectively useful words higher than vocabularies containing words which are subjectively not useful.
However, we saw some exceptions to this observation, which invites further research to be performed in the area of evaluating vocabularies and vocabulary lists.

On the question about list efficiency, our results show that the lists generated with some of our XAI approaches outperform traditional, frequency-based metrics.
The gap is fairly large on small corpora, but small on larger corpora with separate generation and evaluation datasets.
Among the XAI-driven generation methods, Single Token Ablation and Single Token Summary generated the best lists in almost all instances.


Finally, we can answer the question about the appropriate use case of each approach:
The best-performing XAI generation methods --- Single Token Ablation and Single Token Summary with the sentence embedding model --- outperformed frequency baselines, especially on smaller corpora, but require 10 to 20 times the runtime required for the frequency-based approaches.
When the efficiency of generated lists is the most important factor, we therefore recommend their use.
One could also mix the lists to combine their strengths:
Lists generated by Single Token Summary have the best top words among all the approaches, while Single Token Ablation is the best approach for large vocabulary sizes.
Therefore, merging the lists into one such that Single Token Summary words occupy e.g., its top 40 words, then inserting Single Token Ablation words not yet in the list, could be used to combine a stronger list overall.

We see a similar promising combination with attention and TF-IDF, where TF-IDF produces excellent results at low vocabulary sizes, while attention lists are strong for large sizes.
These two approaches produce the next best results after Single Token Summary and -Ablation for small corpora, but have lower runtimes (TF-IDF being on par with raw frequency in terms of runtime, and attention taking about 6 times more time than frequency methods).
Our implementation of the attention list generation approach did not utilize batch processing with the AI model, which could further reduce the time needed for list generation.
Attention and TF-IDF could therefore be used in cases where a low runtime is an important factor in the selection of the generation method.

For generating vocabulary lists for large contexts, raw frequency lists showed among the best scores among all approaches at large vocabulary sizes.
Due to its very time-efficient generation and not needing a normalizing corpus like TF-IDF, this recommends the raw frequency method for the compilation of generally useful vocabulary lists.

With these important insights, the final chapter of this work suggests changes which could be made to our approach to further improve its usefulness to language learners and teachers.


