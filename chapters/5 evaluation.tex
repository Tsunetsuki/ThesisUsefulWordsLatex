Evaluation of the vocabulary list generation approaches is challenging:
We can measure the quality of each approach by measuring the efficiency of the vocabulary lists it produces.
We have described in detail our method for vocabulary efficiency evaluation in Section \ref{sec:experimental-setup-with-ai}, which involves running an AI model on a context-specific corpus with a given vocabulary, where the vocabulary is given by taking the first $k$ words from the vocabulary list in question.
However, this approach is very similar to our approach for list generation, which involves analyzing AI interaction of AI models with corpora through the use of XAI methods.
Therefore, it is natural to assume that our evaluation approach is biased towards the lists stemming from our list generation approach.
This concern is especially pertinent as it has been shown that AI models do not reliably respond comparably to humans when the inputs to NLP tasks are perturbed \cite{tjuatjaLLMsExhibitHumanlike2024}.
However, to the best of our knowledge, there has not yet been a proposed evaluation metric for vocabulary lists that can be done automatically and which outputs a simple numeric result.

To address this concern, in addition to our own evaluation approach, we also present sample vocabulary lists, use our evaluation approach, and check whether the results align with human intuition.

For this reason, our evaluation is not only concerned with analyzing statistics of measured efficiency, but also with whether the calculated efficiency values are reliable.
The questions examined in this chapter can be summed up as:
\begin{enumerate}
	\item Does our efficiency evaluation of our list correspond with human intuition? (see Section \ref{sec:does-eval-correpond-to-intuition})
	\item According to this efficiency evaluation, do the lists generated by our approach outperform lists from frequency-based approaches? (Sections \ref{sec:results-small-context} and \ref{sec:results-large-context})
	\item Taking all data (list performance, similarity, runtime, etc.) into account, which approach is suited best for which situations? Can we combine the approaches to achieve even better results? (Section \ref{sec:results-discussion})
	      % \item How can the various approaches be combined to supplement each other's weaknesses?
\end{enumerate}

\todo{Add section references when chapter structure is fixed}

The following sections first introduce several baselines against which we compare our list generation approaches.
These baselines include both publicly available vocabulary lists from online language learning tools, and existing list compilation methods introduced in Section \ref{sec:frequency-based-list-generation-methods} in connection with the corpora used for our own lists for a more direct comparison.

We then describe the (numeric) metrics used in the evaluation.
Finally, we present the results of the evaluation, as well as discuss their implications.

\section{Baselines}
It is useful to compare the lists generated by our AI-driven vocabulary list generation approaches with both existing word lists from educational materials, and frequency-based compilation approaches which use the same data as our approaches as input.
They serve as a point of comparison, to see where generated lists agree with existing lists, and to compare the efficiencies of the lists with ours.
For this reason, Section \ref{sec:exising-lists} introduces sources of external lists, and Section \ref{sec:freq-generation-methods-baselines} reviews the frequency-based approaches used as baselines.

\subsection{Existing Vocabulary Lists} \label{sec:exising-lists}
As one point of comparison, we can use existing vocabulary lists that are readily available in digital format, regardless of the method which created them.
For this, a survey was performed on popular language learning tools which make public some of their learning data.
However, as most tools do not publish their vocabulary lists free of cost in text form, we only obtained one list for comparison from the online learning platform \Rosetta\, as the following subsections show.

\subsubsection{Rosetta Stone}
\Rosetta \footnote{\url{https://rosettastone.co.jp/}} is a popular online and mobile language learning platform.
\Rosetta\ publishes some of the course contents on its website \footnote{\url{https://support.rosettastone.com/s/article/Rosetta-Stone-Course-Contents}}.
These lessons take the form of phrases intended to help learners to quickly gain competence in their target language.
To make a list of vocabulary from \Rosetta 's lesson contents, we use the freely available lessons  of the \textit{American English} course (Units 1-20).
We convert the PDF files to text format, filter lines which do not contain lessons contents (e.g., "Lesson 8" and similar ancillary notes).
On the filtered text, we run the \texttt{bert-base-uncased} tokenizer, merge the sub-word tokens to words and let the order of first appearance of a word in the lessons be its rank in the resulting vocabulary list.
Thus, the generated list features the lessons' words in the order in which \Rosetta\ introduces them to its learners, which presumably is close to the word order that \Rosetta\ estimates to be most useful.
One qualification of this statement is that some words may be introduced in order for the learner to have something concrete to talk about, rather than because of their estimated utility.
The final vocabulary list contains 2,545 words in total.

\subsubsection{Unsuitable Sources of Vocabulary Lists}
The following sources of vocabulary lists were considered, but not used for the reasons described below.

\paragraph{Textbooks:}
An obvious source of vocabulary lists to compare our lists against are English textbooks, which often contain vocabulary lists for each lesson.
However, the books we considered as candidates do not publish their course contents free of charge.
This includes:
\begin{itemize}
	\item \textit{Practical English Usage} by Michael Swan \footnote{Available at Oxford University Press at \url{https://elt.oup.com/catalogue/items/global/grammar_vocabulary/practical_english_usage_4th_edition/9780194202510} (last accessed on April 25, 2025)}
\end{itemize}

\paragraph{Cambridge Word Lists:}
Cambridge publishes word lists for certain levels of English \footnote{\url{https://www.cambridgeenglish.org/images/149681-yle-flyers-word-list.pdf}}.
However, these lists are meant for assessing learner's level of vocabulary skills, and the list for each level is ordered by alphabetical order, rather than the perceived level of the word, making them unusable as comparison to our approach.

\paragraph{Duolingo:}
While \textit{Duolingo} \footnote{\url{https://duolingo.com/}} is a popular language platform, it does not publish its course contents in a format that is easily convertible to text and thus be converted to lists of vocabulary.

% This section has introduced some pre-existing lists as points of comparison for our generated vocabulary lists.
% However, one shortcoming of this comparison is that these lists have not been prepared with the same linguistic contexts in mind as those made by this work's approach, and thus their utility is naturally greater in general-context corpora than specialized ones.
% While the context specificity of this work's utility extraction method is a major advance for creating vocabulary lists, it also makes this an unfair comparison.
%
% However, apart from pre-existing lists, there also exist pre-existing \textbf{methods} of finding important words in texts, which we can also use as baselines to compare with our method.
% These also use corpora, but not AI models or XAI methods to find important words.
% The next section introduces some of these methods, which we use to generate comparison lists to our own.


\subsection{Frequency-Based List Generation Methods} \label{sec:freq-generation-methods-baselines}
To gain an impression of how well our XAI-based method for vocabulary list generation performs, we employ the three frequency-based methods for generating lists of vocabulary introduced in Section \ref{sec:frequency-based-list-generation-methods}, namely Raw Frequency, Average Reduced Frequency, and TF-IDF.
We use these three measures as baselines to our methods, again using the tokenizer \texttt{bert-base-uncased} (and merging tokens into words) to divide texts into words that we can count.
To calculate the inverse document frequency in TF-IDF, we use a random sample of 53,178 documents from the Oscar corpus (see Section \ref{sec:oscar}).

One can view these frequency approaches as assigning corpus utility to words in the same way that our AI-driven approaches do, with the corpus utility of a word being given by its frequency/average reduced frequency or TF-IDF.
Therefore, for the large context scenario, their utility aggregation is also performed by summing over the corpus utilities for the single corpora in the large context, as explained in Section \ref{sec:impl-summary-description}.

In this section, we have introduced our baselines, namely the \Rosetta\ vocabulary list, and the three frequency-based list generation methods.
The next section will discuss how we evaluate these baselines, as well as our own various approaches and lists.

% \subsection{LLM Prompt}
% In recent years, Large Language Models have become a popular tools for language learners to find new words to learn about specific areas.
% For this purpose, we run the following prompt to ChatGPT 3 for each context to get a sense of how well our method performs against this easy method.
%
% Prompt:
%
% \begin{lstlisting}[caption={Prompt given to the language model.}, label={lst:blade_runner_prompt}, captionpos=b]
% 	Given a learner of English who knows no English words so far:
% Give me a list of 50 words from the script of the movie "Blade Runner" that they can learn, such that when reading the full script, they have the best understanding of what they are reading.
% Order the list such that the most useful words appear at the top.
% Do not group the words, just generate a raw list.
% Do not number the bullet points. Do not put hyperlinks in the list.
%
% \end{lstlisting}
% \todo{results}


\section{Evaluation Measures}
The various list generation extraction approaches produce ranked lists as outputs.
To compare these, we employ our evaluation approach as described theoretically in Section \ref{sec:ai-as-test-subject}.
Section \ref{sec:task-performance-measure} discusses the implementation of this approach.
In addition, we compare the lists by how similar they are to each other in Section \ref{sec:list-similarity-measure}.

\subsection{NLP Task Performance} \label{sec:task-performance-measure}
Our primary metric for evaluating a vocabulary list's efficiency is the approach explained in Section \ref{sec:experimental-setup-with-ai}:
We let an AI model perform an NLP task on a single corpus, using a vocabulary $V_{l, k}$ which consists of the first $k$ elements of the list $l$.
The limited vocabulary is simulated by removing words outside the vocabulary from the inputs to the model.
For illustration, consider again the example sentence:

\begin{quote}
	\textit{Abraham Lincoln faced enmity in 1863.}
\end{quote}

If our vocabulary consists only of the word \textit{enmity}, this sentence becomes:

\begin{quote}
	\textit{Abraham Lincoln enmity 1863.}
\end{quote}

Since \textit{Abraham Lincoln} is recognized as a named entity, we leave it in the input, even though it is not part of the vocabulary.
Non-alphabetic tokens, such as numbers and punctuation, are also left in the input.

Listing \ref{alg:efficiency-evaluation} shows pseudocode for our implementation of the list evaluation approach:

\input{listings/efficiency_evaluation_pseudocode.tex}

We start the evaluation of every vocabulary list with a vocabulary size of 0, letting the model run on inputs where only named entities and non-alphabetic tokens are left.
The score for the entire corpus is calculated by taking the arithmetic mean of the scores of individual lines.
These runs with a vocabulary size of 0 lets use observe the "utility" of the named entities, putting into context the performance scores when vocabulary is added.
After this, we set the vocabulary size to 10 words, and run the AI models with the resulting vocabulary $V_{l, 10}$.
We then progressively increase the vocabulary by increasing index $k$ by a factor of 4, until we reach the end of the list.
The increase of vocabulary size is exponential, as this ensures a quick evaluation and performance mostly increases at the start of the list, not grow very much towards its end (this is confirmed by the evaluation results shown in Sections \ref{sec:results-small-context} and \ref{sec:results-large-context}).

As stated in Section \ref{sec:sentence-embedding}, we employ the sentence embedding model in the evaluation.
We measure the performance of the model for a sentence variation (with only the words in the vocabulary) by how close the output vector is (in cosine distance) to the output vector of the unaltered sentence.
In order to let the score take on values between 0 and 1, we normalize the line score by also taking the distance between the baseline vector and the model output for an empty string (\texttt{""}).

This approach gives us a direct, quantitative metric for the efficiency of a vocabulary list at each of the test vocabulary sizes.
As an illustration, we present the results of one such efficiency evaluation run for three vocabulary lists on the Wikipedia Article of American actor Marlon Brando \footnote{\url{https://en.wikipedia.org/wiki/Marlon_Brando}} in Figure \ref{fig:marlon-brando-eval}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{marlon-brando-eval.png}
	\caption{Efficiency evaluation results on vocabulary lists for the Wikipedia Article of Marlon Brando.}
	\label{fig:marlon-brando-eval}
\end{figure}


To make the figure readable, only three list generation approaches are compared, namely Raw Frequency, TF-IDF, and Single Token Ablation with the sentence embedding model.
We first evaluate the AI model performing the task with a vocabulary size of zero, simulating the state of not knowing any words.
The only tokens in this run are named entities and punctuation, as these are not considered to be potential vocabulary terms.
The vocabularies are then evaluated with an initial size of 10, and increase by a factor of 4 in each step, until a final evaluation is done with the full vocabulary list.
For single corpora, the full list contains all words from the corpus, which is why the performance reaches 1 with all lists.

To compare the results on a broader scale, we aggregate the results of the evaluation runs for each list generation evaluation method as listed in Section \ref{sec:impl-summary-description}.
For both the small context and the large context scenario, we take the average across the lists' scores across the single corpora.
The difference in the scenarios in the evaluation is that for the small context, a list is individually compiled from and evaluated on each single corpus in the large context, whereas for the large scenario, the same vocabulary list is evaluated on all single corpora in the evaluation set of the context (see Section \ref{sec:final-dataset}).

This section has described our approach to evaluating the efficiency of vocabulary lists.
However, as this approach is a novel one, we also compare the \textit{similarity} of the vocabulary lists as a sanity check.
The next section will introduce the metrics we use for this comparison.

\subsection{List Similarity} \label{sec:list-similarity-measure}
Apart from our own vocabulary list evaluation approach that measures the performance of an AI model using the vocabulary list, we also compare how similar the lists are in general.
This allows us to check whether the results of our efficiency evaluation are plausible.
It also serves another purpose, for the case when two approaches for list generation produce vocabulary lists of similar efficiencies, but one takes much less time:
If the lists of both approaches are not only similar in efficiency, but also in content, we can safely say that the approach taking less time is objectively the superior one (for the data observed).

In order to compare the similarity of the resulting vocabulary lists provided by our approaches, metrics are needed that can be consistently calculated across the different approaches.
While a human may be able to qualitatively analyze lists and gain a rough idea of their similarity, computed metrics provide an instantaneous (if simplified) outlook on similarities.
By a metric, we mean a function which takes as parameters two ranked word lists of equal length and outputs a real number giving either a distance or similarity between the lists.

Two considerations influenced our choice of the similarity metrics we employed in our evaluation:
Firstly, the metrics must be \textbf{able to handle elements which occur in only one of the two lists}.
Secondly, differences between elements at \textbf{the tops of the lists must impact the metric more} than differences between elements at the ends.
This is because the top of each vocabulary list contains the words which are ranked as most important.

While Kendall rank correlation \cite{kendallNewMeasureRank1938c} and Spearman's footrule  \cite{spearmanCorrelationCalculatedFaulty1910} are simple metrics to compare ranked lists, they make no distinction between differences at the tops of lists and differences at their ends, in addition to being difficult to adapt to handle for missing elements.
Instead, we use two metrics called Average Overlap and Normalized Discounted Cumulative Gain, which we introduce in the following subsections.
%
% \subsubsection{Sequential rank agreement (modified)}
% Sequential rank agreement \cite{ekstromSequentialRankAgreement2015} is based on the deviations of some subset of the lists in the upper ranks.
% 	      It is important to note that this metric has an additional parameter "depth" which determines how many elements (from the top of the list) are considered.
% 	      It is therefore more helpful to view its results at various depths.
% 	      The original formula for this metric in the case of two lists is:
% 	      \[
% 		      a_{d} := a \text{ from start to rank } d
% 	      \]
%
% 	      \[
% 		      S_{d} := a_{d} \cup b_{d}
% 	      \]
%
% 	      \[
% 		      SRA_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \left( r_{b}(x) \right) - \left( r_{a}(x) \right) \right)}{|S_{d}|}
% 	      \]
%
% 	      where \(\lambda\) is a normalization factor ensuring that \(\max(SRA) = 1\).
% 	      In its proposed form, this metric can only compare lists which contain the same set of unique elements, just in different orders.
% 	      In order to make it work on lists where this is not the case, one can set the "rank" of nonexisting elements to a value greater than the length of the lists, such as \(2 |a|\).
% 	      Another drawback of the metric is that the standard deviation of two numbers does not depend on their absolute value, only their difference.
% 	      However, to satisfy number 3 of the stated requirements, we can take the deviation of the logarithm of the ranks instead of the deviation of the ranks themselves, resulting in the formula
%
% 	      \[
% 		      r^{\prime}(x) :=
% 		      \begin{cases}
% 			      \mathrm{rank}_{b}(x) & \text{if } x \in b, \\
% 			      2 \cdot |a|          & \text{otherwise.}
% 		      \end{cases}
% 	      \]
%
% 	      \[
% 		      SRA^{mod}_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \log (r^{\prime}_{b}(x))-\log(r^{\prime}_{a}(x))\right)}{|S_{d}|}
% 	      \]
%
% 	      For this modified version, \(\lambda\) can be calculated with:
%
% 	      \[
% 		      \lambda = \frac{1}{SRA_{d}(a, a^{*})},
% 	      \]
%
% 	      where \(a^{*}\) is a list such that \(a \cap a^{*} = \emptyset\).

\subsubsection{Average Overlap}
The Average Overlap \cite{webberSimilarityMeasureIndefinite2010} of two lists is given by the average cardinality of the intersection (overlap) of the sets given by taking the two lists up until every possible index.
Because the first element contributes to every iteration, it causes the tops of the lists to impact the metrics more than their ends.
There is also no issue with partial overlaps between the lists.
Using the notation of a vocabulary $V_{l, k}$ as a set of words given by vocabulary list $l$ until an index $k$ (see Section \ref{sec:voc-list-efficiency}), the average overlap between two vocabulary lists like is given by:

\begin{equation}
	\text{AverageOverlap}(A, B) = \frac{1}{n} \sum_{k=1}^{n} \frac{|V_{A, k}  \cap V_{B,k}|}{k}
\end{equation}

% \item \textbf{11-point interpolated average precision} \cite{manningIntroductionInformationRetrieval2008}: Uses set metric precision (though may also use recall or F1 score on various subsets of the list (first 10\%, first 20\% etc.) and takes their geometric mean to arrive at a single number.
%       As each of the elevent numbers is calculated on the partial subset of the list's elements starting at the first element, this means that changes in the top of the list affect more of these numbers and thus have a larger impact on the final calculated mean.
%       One drawback of this method is that precision measurement within the 10\% interval only takes into account set membership, not the order of words:
%       For a list containing 10,000 words, the evaluated intervals are words in the index intervals $\left[ 0, 1000 \right), \left[ 0, 2000 \right), ... ,\left[ 0, 10000 \right)$.  Differences of order within the first 1000 words are thus ignored.

\subsubsection{Normalized Discounted Cumulative Gain (NCDG)}
(Normalized) Discounted Cumulative Gain \cite{jurafskySpeechLanguageProcessing2025} is a metric used in information retrieval to compare relevance scoring of two sets of elements.
By making the relevance score $R$ of a set element determined by its rank in our ranked lists, we adapt this metric to compare the similarity of two lists:

% This formula outputs a value between 0 and 1, with 1 being given if both lists are identical, 0 when they have no elements in common, and values in between when there is partial overlap between elements and/or their order is different.

% ${DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}=rel_{1}+\sum _{i=2}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$



\[
	\text{DCG}(A, B) = \sum_{k=1}^{n} \frac{2^{R(A_k)} - 1}{\log_2(1 + k)}
\]

The relevance $R_B$ is a function taking as input an element (not an index) of list A.
We define it as follows:

\[
	R_B(m) :=
	\begin{cases}
		\frac{1}{rank_{B}(m) + 1} & \text{if } B \text{ contains } m \\
		0                         & \text{otherwise}
	\end{cases}
\]
With relevance defined in this way, we ensure both that differences between the tops of the lists impact the measurement more than differences at the end, and the handling of elements which only appear in one of the two lists.
To normalize this metric to output values between 0 and 1, we divide by the maximum value for the list length, which is given by the DCG of one of the lists with itself:

\[
	\text{NDCG}(A,B) = \frac{1}{\text{DCG}(A, A)} \text{DCG}(A, B)
\]

Having established our metrics to evaluate both the performance of and similarity between vocabulary lists, we present the results of the evaluation in the following section.

\section{Results} \label{sec:results}
This section presents the results both for the small context and large context scenarios.
The list generation approaches are evaluated by the performance of their lists on NLP tasks, by the similarity between the lists, and by the time required to generate them.

We first establish that our approach to evaluating list efficiency does roughly align with human intuition in Section \ref{sec:does-eval-correpond-to-intuition}.
After this, we show the results for approaches in the small context scenario where lists are compiled and tested on single Wikipedia articles and subtitles in Section \ref{sec:results-small-context}, where we find that our AI-driven approaches generally outperform the baselines.
Finally, the results for the large contexts (using separate single corpora for generation and evaluation) are presented in Section \ref{sec:results-large-context}.
For the large contexts, we find that most list generation approaches produce lists of similar efficiencies, which seems to imply that frequency is a valid proxy metric for large vocabulary sizes, as it requires less time for vocabulary list generation. 

\subsection{Correspondence of Efficiency Evaluation with Human Intuition} \label{sec:does-eval-correpond-to-intuition}
As mentioned in the introduction to this chapter, we use the performance of AI models on corpora where we simulate a limited vocabulary in order to measure the efficiency of a vocabulary list.
Before we present the results of these tests, however, we examine whether the performance of AI models is a reliable indicator that the list is useful to humans as well.
This section therefore examines several vocabulary lists tested on a short sample text containing ten lines of dialog from the subtitles of the 1972 movie \textit{The Godfather}, seen here:

\input{data/sample_text_original.tex}

The following shows the text leaving only the named entities and punctuation tokens, which constitutes our starting point for the evaluation.
An evaluation score for each line is displayed on the left:

\input{data/sample_text_empty.tex}

Due to the uncased tokenizer used, all words in the reconstructed text are lowercase.
We can see that the lines containing named entities (Line 4 and 9) already have a rather high score in the evaluation, reflecting the importance of the names to the meaning of the sentences.
The other lines receive a score of 0.0, resulting in an overall score of 0.17.
We also observe that the name \textit{Moe} in line 1 is not recognized by the Named Entitiy Recognition model, (falsely) making it a possible vocabulary word.

To compare their intuitive utilities, we use four vocabulary lists on the corpus and see how well their first words perform when evaluated on the sample text:
\begin{enumerate}
	\item A randomly ordered list from words in the sample text.
	\item A list compiled with raw frequency.
	\item A list compiled with TF-IDF (with the Oscar corpus as normalization).
	\item A list compiled by Single Token Summary with sentence embedding.
\end{enumerate}

The tops of the lists can be seen in Table \ref{tbl:first-k-words-sample}.

\begin{table}[H]
	\centering
	\input{data/first k words sample.tex}
	\caption{Top 10 words of each compared list on the sample text.}
	\label{tbl:first-k-words-sample}
\end{table}

It can be seen that the ten top words of the lists collected with TF-IDF and the Single Token Summary approach give a better idea of the meaning of the text than the words from the random and raw frequency lists.
Next, we examine the words in the context of the sample text.
In the following listings, we see the sample text with the 10 top words filled in from each vocabulary list, in addition to the recognized named entities.
Each line displays the evaluation score for it, and the total score (calculated by taking the average across the lines) is seen in the caption of each listing:

\input{data/sample_texts.tex}

The main observation is that the texts with the TF-IDF and Single Token Summary, whose top words make the meaning of the text much closer to the meaning of the original text than those of the other two approaches, are indeed evaluated significantly higher (0.52 and 0.56 vs. 0.33 and 0.30).
In our opinion, this is evidence that our evaluation scores roughly coincide with how much the words let a reader understand the text, aligning with utility as defined in Section \ref{sec:utility}.
Both of these lists feature the essential words \textit{casino}, \textit{godfather} and \textit{gamblers}, as well as other words which let a reader guess at the situation.

We can, however, also notice some peculiarities in the evaluation scores:
The text with words from the randomly assembled list (Listing \ref{lst:sample-text-random}) seems more understandable to us than when words from the  frequency list are filled in (Listing \ref{lst:sample-text-frequency}), since the randomly compiled list contains useful words such as \textit{selling}, \textit{casino} and \textit{contract}.
However, the text with the frequency words receives a higher evaluation score than the one with the random words (0.33 vs. 0.3).
This is due partially to the unrecognized entity \textit{moe}, which is contained in the frequency list but not the random one, "artificially" boosting the score of the frequency list.
But even disregarding that name, the scores are not as disparate as we expected them to be:
For example, line 3 in the random words text \texttt{"i' make       ."} receives a (rounded) score of 0.0, while the same line in the frequency text \texttt{"i' ll      he      ."} is assigned a score of 0.3.
Since both versions of the line seem to contain very little information, this difference in the evaluation score suggests that the cosine distances between the sentence embeddings do not completely correlate with differences in meaning of the sentences, and thus the evaluation scores may not reflect the true utility of words in all circumstances.
With these observations in mind, we present the evaluation results in the next sections.

\subsection{Small Context} \label{sec:results-small-context}
This section shows the results of our evaluation approach for the efficiencies of vocabulary lists, applied to lists generated by the various list generation approaches.
Performance scores were rounded to two decimals to improve readability.

Table \ref{tbl:performance-results-single-opensubs} and \ref{tbl:performance-results-single-wikipedia} show the aggregated results of single corpora, separated by corpus type.
For the small context scenario, a vocabulary list is produced for each single corpus and extraction method, whose efficiency is then evaluated with the same single corpus as a basis.
%
% \begin{table}[ht]
% 	\centering
% 	\input{data/performance.tex}
% 	\caption{DELETE THIS}
% \end{table}


\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/performance opensubs-subtitle single.tex}
	}
	\caption{Model performance across vocabulary sizes on single subtitles.}
	\label{tbl:performance-results-single-opensubs}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/performance wikipedia-article single.tex}
	}
	\caption{Model performance across vocabulary sizes on single Wikipedia articles.}
	\label{tbl:performance-results-single-wikipedia}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_single average-overlap.tex}
	}
	\caption{Average Overlap similarity of small context vocabulary lists. Black boxes represent missing values.}
	\label{tbl:similarity-results-single-average-overlap}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_single ndcg.tex}
	}
	\caption{NDCG similarity of small context vocabulary lists. Black boxes represent missing values.}
	\label{tbl:similarity-results-single-ndcg}
\end{table}





We can observe several points in the data:

Single Token Ablation with sentence embedding achieves the highest results across corpus types.
It is also among the approaches with the least spread of performance, as seen by the standard deviation of the corpora scores.
Since the model used for list generation and list evaluation is the same,
What we can gleam from this fact is that the utilities calculated from individual lines, when aggregated over the dataset, approach the word utilities with respect to the entire corpus better than frequency-based approaches.

The Progressive Summary approach, despite being a more complex approach, does not outperform Single Token Ablation.
The comparison is difficult, as the calculation of line utilities for Progressive Summary is not as straightforward as for Single Token Ablation.
However, Progressive Summary has the additional disadvantage of requiring a much greater amount of time to calculate for a corpus.
For this reason, it seems that investigating possible improvements for how line utilities can be aggregated to calculate corpus utility may not be productive.

Single Token Summary produced the lowest performance across corpus types and vocabulary sizes, falling short of even the frequency-based baselines.
This may be due to the fact that it does not take into account relationships between words in the sentence, as each summary contains only a single word.
For this reason, Single Token Summary seems to be of very limited potential for word utility evaluation.

TF-IDF, despite its simplicity, produces lists of an efficiency very close to the XAI methods, especially at low vocabulary sizes.
TF-IDF, while usually used to capture words characteristics of documents, also seems to capture useful words rather well.
This could be seen as evidence to the effect that, while learning words by their general frequency provides the best coverage of texts, the coverage is not a good proxy for measuring the understanding of a text.


For the transformer attention approach, both the model for next sentence prediction and the model for sentence embedding produce very similar results.
Because both models function with a BERT model at their core, this result is within the realm of expectation.
The slightly better performance of the NSP model may be due to its greater size (12 attention heads vs. 6 heads), but the difference is too slight to make a definitive judgement.

We can also observe that the type of corpus used had a significant impact on the model performance for small vocabulary sizes:
The average performance of 10- and 40-word-vocabularies was $0.37$ and $0.53$ for Wikipedia articles, but only $0.16$ and $0.30$ for subtitles, respectively.
The discrepancy in performance of small vocabulary sizes between the two corpus types may be due to the fact that Wikipedia articles address a particular \textit{topic}, meaning that knowing the main words (such as those from the article title) may produce a high initial boost in performance.
One piece of counter-evidence to this theory is that raw frequency also performs better on Wikipedia than on OpenSubtitles, despite frequency lists from both corpus types featuring many stopwords at their top.
An alternative hypothesis is that Wikipedia articles are more similar to the data the models had been trained on.
This cannot be confirmed, however, as the source of BERT's training data is not publicly known.
An interesting point to note is that, for a vocabulary size of 640, the list efficiencies nearly converge:
Despite the larger average corpus size of movie subtitles, the average performance is 0.90 for subtitles and 0.89 for Wikipedia.

\subsubsection{Speed} \label{sec:eval-speed}
When evaluating the broad applicability of a vocabulary list generation approach, the efficiency of generated vocabulary lists is not the only criterion:
The runtime of the approach is also an important factor.
Table \ref{tbl:runtime-results} shows the average runtime for the approaches per corpus type.
It must be kept in mind that the runtime is not only influenced by the word utility evaluation method, but also by implementation details, such as whether batch processing of inputs by the AI models is exploited, and the size of the model employed.
Named Entity Recognition is performed within each method, and contributes most of the runtime for the frequency-based approaches.

\begin{table}[ht]
	\centering
	\input{data/runtimes.tex}
	\caption{Runtime of list generation approaches.}
	\label{tbl:runtime-results}
\end{table}

As is to be expected, the frequency-based approaches are orders of magnitude faster than the XAI-based approaches.
Among the XAI approaches, we can make several observations:
First, there is a marked difference in runtime between the NSP model and the sentence embedding model.
This seems to be due both to the greater complexity of the NSP model, and the fact that it processes more data in one call (a pair of lines instead of a single line).
Attention is the fastest list generation approach, owing to the fact that only one model call is necessary per line explanation.
Attention could be made even faster with batch processing, but since it is a fairly efficient method already, we did not optimize it to the fullest extent.

On average, most of the XAI approaches take more time on subtitles than one Wikipedia articles, with the exceptions of Progressive Summary.
This aligns with our understanding of the corpora and the approaches \ref{tbl:corpus-sizes}:
Wikipedia Articles have fewer lines, but more words per line, making the Progressive Summary approach take longer due to its time complexity of $\mathcal{O}(n^2)$.



\subsection{Large Context} \label{sec:results-large-context}
So far, we have analyzed the efficiencies of vocabulary lists with respect to the same corpus which was used to generate them in the first place.
This may be useful for a learner who wishes to read specific articles of watch specific movies, and learn the most useful vocabulary beforehand.
However, in real life, it is not always possible for a learner to know the exact contents of the texts they will be engaging in:
Natural conversations, for examples, are not deterministically predictable, and learners will likely wish to acquire abilities beyond the narrow scope of a few selected Wikipedia articles.
For this reason, we also test the efficiencies of vocabulary lists on corpora which are similar, but not the same as the source of their generation.

To model these linguistic contexts, we use Wikipedia categories:
For each category, we randomly split its articles into "train" and "test" articles, with a test percentage of 20\%.
When generating vocabulary lists, only the "train" articles are used as inputs.
With these lists, we evaluate the list efficiencies against the "test" corpora.
The scores across the individual corpora is then aggregated by taking the mean.
% Table \ref{tbl:performance-results-wikipedia-categories} shows the results of these experiments.

To first gain an overview of the resulting lists, we present the first 20 words of each vocabulary list made from the sample of 632 subtitles:

\begin{table}[H]
	\centering
	% \resizebox{\textwidth}{!}{%
	\input{data/first k words OPENSUBS.tex}
	% }
	\caption{Top 20 words of the generated lists for the OpenSubtitles dataset.}
	\label{tbl:first-k-words-opensubs}
\end{table}

We can see some similarities across all lists:
Every list is headed by the two pronouns \textit{i} and \textit{you}, which seems reasonable for movie dialog.
After these, we can see the characteristics of the various approaches:
Raw Frequency and Average Reduced Frequency produce very similar lists, consisting mostly of stop words at the top.
The list made from using attention as explanation follows a similar pattern.
In contrast, the lists generated from both TF-IDF and feature importance explanations show words that are more useful for a beginner language learner in our view:
All of them contain at least one verb (\textit{know}), as well as several words that can constitute a meaningful sentence by themselves, such as \textit{what}, \textit{okay}, \textit{right}, \textit{yeah}.
Next, we present the results of the efficiency evaluation of the large lists produced on the (separate) test datasets.
We include the vocabulary list produced from the \Rosetta\ lesson in this evaluation, since both it and our own lists were not specifically compiled for the evaluation data (though our lists were compiled on more similar data).


\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/performance opensubs-subtitle big.tex}
	}
	\caption{Model performance across vocabulary sizes on subtitles, with separate train/test subtitles.}
	\label{tbl:performance-results-opensubs-large}
\end{table}


\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/performance wikipedia-article big.tex}
	}
	\caption{Model performance across vocabulary sizes on Wikipedia articles, with separate train/test articles.}
	\label{tbl:performance-results-wikipedia-large}
\end{table}

We can see that the results for most approaches achieve \textbf{very similar} evaluation scores, with difference of at most 0.02 per tested vocabulary size.
The only exception is the \Rosetta\ list, which performs significantly worse than either the frequency or the XAI lists.
This is a surprising result, as the subjective evaluation seen above would seem to imply that the lists compiled with feature attribution methods should outperform the frequency-based metrics.
It can be seen that the Single Token Summary approach still outperforms the other approaches for small vocabulary sizes, which may be due to it selecting words which, by themselves, are closest to the meaning of an entire sentence.

However, we take the small differences in these result as an indication that, \textbf{for large contexts, frequency-based approaches are a viable way to collect vocabulary}, with more complicated methods only bringing marginal improvements to vocabulary list efficiency.
The fact that the datasets for generating the lists and evaluating them is a likely cause for why these results are similar, as finding words that are very specifically useful to the "train" dataset becomes less productive in such a scenario.
Still, the TF-IDF metric can be seen to achieve better results than raw frequency, and comparable results to the XAI approaches, which makes it a worthwhile method for compiling context-specific word lists in our view, especially since it requires much less time to be run on a large corpus.
The following tables show the similarities of the generated lists.


\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_big average-overlap.tex}
	}
	\caption{Average Overlap similarity of large context vocabulary lists. Black boxes represent missing values.}
	\label{tbl:similarity-results-big-average-overlap}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\input{data/similarity_heatmap_table_big ndcg.tex}
	}
	\caption{NDCG similarity of large context vocabulary lists. Black boxes represent missing values.}
	\label{tbl:similarity-results-big-ndcg}
\end{table}





% \begin{table}[ht]
% 	\centering
% 	\input{data/performances_wikipedia_categories.tex}
% 	\caption{Performance of Vocabulary Lists on similar corpora.}
% 	\label{tbl:performance-results-wikipedia-categories}
% \end{table}


% \begin{table}[ht]
% 	\centering
% 	\resizebox{\textwidth}{!}{%
% 		\input{data/performances big all.tex}
% 	}
% 	\caption{Performance of Vocabulary Lists on similar corpora, for each Wikipedia category.}
% 	\label{tbl:performance-results-big-all}
% \end{table}

\input{data/performances big all.tex}
Because vocabulary lists are generated from a much larger corpus, they are longer than the per-article lists in section \ref{sec:results-small-context}.
We can also observe that the increase in performance is much slower than when a list is evaluated on the same corpus used to generate it.


\section{Discussion} \label{sec:results-discussion}
Our results show that the lists generated with our XAI approach, using feature importance methods, outperform traditional, frequency-based metrics on small corpora, and slightly on larger corpora for small vocabulary sizes.
Especially Single Token Ablation and Single Token Summary generate better lists for small corpora than raw frequency and average reduced frequency, while still only requiring about ten times the runtime require for the frequency-based approaches.
Attention, at least with our setup, does not produce comparably performant lists.
Progressive Summary, while being similar to Single Token Ablation and Single Token Summary in its results, did not outperform them despite its much longer runtime.

For smaller corpora, therefore, we recommend the use of the Single Token Ablation and Single Token Summary to create maximally useful word lists for language learners.

For larger corpora, one could combine the performance of the Single Token Summary approach with the speed of a frequency-based approach in the following way:
For a large sample of the corpus, one could use the Single Token Summary to create the top of the list.
This list head would be likely to include non-stop words, which a learner could use to form grammatically correct sentences from the start.
After this top portion of the list, one could use TF-IDF to generate the rest of the list, avoiding the long tail of frequency for rare words.


\todo{Interpretation,
	Could you combine approaches with each other or baselines to  combine strength, what are the strengths/weakness of each approach}





