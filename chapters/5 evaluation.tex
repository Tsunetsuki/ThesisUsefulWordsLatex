\section{Evaluation Measures}

The various utility extraction approaches produce ranked lists as outputs.
To compare these, we employ both quantitative and qualitative comparison approaches, described in the following chapters.

\subsection{List Similarities}
In order to compare the results provided by the various approaches, metrics are needed that can be consistently calculated across the different approaches.
While a human may be able to qualitatively analyze lists and gain a rough idea of their similarity, computed metrics provide an instantaneous (if simplified) outlook on similarities.
A metric shall be defined as a function which takes as parameters two word lists of equal length which are word lists ordered descendingly by supposed relevance, and outputs a real number giving either a distance or similarity between the lists.

Considerations of the choice of metric are:

\begin{description}
	\item [Handling of lists with partial overlap.]
	      Metrics must be able to handle elements which occur in only one of the two lists.
	      Thus, a metric which solely compares ranks of elements is not viable.
	\item [Start of lists is more impactful than end.]
	      Since the beginning of lists contains the words which are ranked as most important, changes at the top should impact the metric more than changes at the bottom. This includes (1) Equal differences in rank should be counted as more important if they occur further up the list.
	      A word that is rank 1 in list A but rank 101 in list B says more about the similarity than if a word is rank 2000 in list A but rank 2100 in list B. Likewise, if a word is absent from list B, it implies a greater difference if that word is at rank 1 in list A than if it were at rank 1000.
\end{description}

\subsubsection{Metrics Used}

\begin{description}
	\item [Sequential rank agreement (modified)] \cite{ekstromSequentialRankAgreement2015}: This metric is based on the deviations of some subset of the lists in the upper ranks.
	      It is important to note that this metric has an additional parameter "depth" which determines how many elements (from the top of the list) are considered.
	      It is therefore more helpful to view its results at various depths.
	      The original formula for this metric in the case of two lists is:
	      \[
		      a_{d} := a \text{ from start to rank } d
	      \]

	      \[
		      S_{d} := a_{d} \cup b_{d}
	      \]

	      \[
		      SRA_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \left( r_{b}(x) \right) - \left( r_{a}(x) \right) \right)}{|S_{d}|}
	      \]

	      where \(\lambda\) is a normalization factor ensuring that \(\max(SRA) = 1\).
	      In its proposed form, this metric can only compare lists which contain the same set of unique elements, just in different orders.
	      In order to make it work on lists where this is not the case, one can set the "rank" of nonexisting elements to a value greater than the length of the lists, such as \(2 |a|\).
	      Another drawback of the metric is that the standard deviation of two numbers does not depend on their absolute value, only their difference.
	      However, to satisfy number 3 of the stated requirements, we can take the deviation of the logarithm of the ranks instead of the deviation of the ranks themselves, resulting in the formula

	      \[
		      r^{\prime}(x) :=
		      \begin{cases}
			      \mathrm{rank}_{b}(x) & \text{if } x \in b, \\
			      2 \cdot |a|          & \text{otherwise.}
		      \end{cases}
	      \]

	      \[
		      SRA^{mod}_{d}(a, b) := \lambda \cdot \frac{\sum_{x \in S_{d}} \sigma ^2 \left( \log (r^{\prime}_{b}(x))-\log(r^{\prime}_{a}(x))\right)}{|S_{d}|}
	      \]

	      For this modified version, \(\lambda\) can be calculated with:

	      \[
		      \lambda = \frac{1}{SRA_{d}(a, a^{*})},
	      \]

	      where \(a^{*}\) is a list such that \(a \cap a^{*} = \emptyset\).
	      % \item \textbf{Average overlap / rank-biased overlap} \cite{webberSimilarityMeasureIndefinite2010}: Compares ranked list and puts more emphasis on the top of the list than at the bottom. However, assumes that the elements of both lists are the same, i.e., that all elements of list A are also in list B and vice versa.
	      %
	      % \item \textbf{11-point interpolated average precision} \cite{manningIntroductionInformationRetrieval2008}: Uses set metric precision (though may also use recall or F1 score on various subsets of the list (first 10\%, first 20\% etc.) and takes their geometric mean to arrive at a single number.
	      %       As each of the elevent numbers is calculated on the partial subset of the list's elements starting at the first element, this means that changes in the top of the list affect more of these numbers and thus have a larger impact on the final calculated mean.
	      %       One drawback of this method is that precision measurement within the 10\% interval only takes into account set membership, not the order of words:
	      %       For a list containing 10,000 words, the evaluated intervals are words in the index intervals $\left[ 0, 1000 \right), \left[ 0, 2000 \right), ... ,\left[ 0, 10000 \right)$.  Differences of order within the first 1000 words are thus ignored.

	\item [Discounted Cumulative Gain]:
	      This formula outputs a value between 0 and 1, with 1 being given if both lists are identical, 0 when they have no elements in common, and values in between when there is partial overlap between elements and/or their order is different.
	      ${DCG_{p}} =\sum _{i=1}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}=rel_{1}+\sum _{i=2}^{p}{\frac {rel_{i}}{\log _{2}(i+1)}}$
\end{description}

\[
	rel_{i} :=
	\begin{cases}
		\frac{1}{rank_{b}(el_{i}) + 1} & \text{if } el_{i} \in b \\
		0                              & \text{otherwise}
	\end{cases}
\]
\subsubsection{Metrics Rejected}
\begin{description}
	\item [Kendall rank correlation ] \cite{kendallNEWMEASURERANK1938b}: This metric is bounded between 0 and 1 and compares the ranks of the elements of two lists. However, it cannot handle elements that only occur in one of the two lists, and thus is not suitable for our purposes. It also does not distinguish between differences in the upper and lower parts of the lists.
	\item [Spearman's footrule] \cite{spearmanCorrelationCalculatedFaulty1910}: Rejected for the same reasons as Kendall rank correlation.
\end{description}


\subsubsection{Tests of Applicability}
[Results of preliminary tests of various metrics on own lists such as rank switching ,replacement at bottom and top of list etc.]


\section{Baselines}
It may be useful to compare the lists generated by the various approaches with existing word lists from educational materials:
Textbooks often feature chapters with word lists, or sentences which can be converted to word lists with a tokenizer.
The purpose is to have a point of comparison, to see if generated lists agree with existing lists, and find reasons for differences.

\subsection{Existing Lists}
\begin{itemize}
	\item Language learning textbooks

	\item Language learning applications
	      \begin{itemize}
		      \item Duolingo: While Duolingo is the most popular language learning application as of 2024, it does not publish its word lists or course contents that is free of cost and easily convertible to a format that can be processed with NLP tools.
		      \item Rosetta Stone: Rosetta Stone publishes Course contents on its website. While the contents take the form of sentences, these can be converted to word lists by using a tokenizer on the contents and creating a list in the order in which they appear in the texts.
	      \end{itemize}
\end{itemize}

\subsection{Existing Methods}
To get an impression of how well the XAI-based methods perform, it will be informative to test established methods such
\begin{itemize}
	\item Frequency: The frequency of the word in the context-specific corpus.
	\item Frequency without stopwords: The frequency of the word in the context-specific corpus, but with known stopwords filtered out.
	\item TF-IDF: The frequency of a word in the context-specific corpus, normalized against a background corpus.
\end{itemize}

\subsection{LLM Prompts}
In recent years, Large Language Models have become a popular tools for language learners to find new words to learn about specific areas.
For this purpose, we run the following prompt to ChatGPT 3 for each context to get a sense of how well my method performs against this easy method.
\todo{Prompt and results}

\section{Results}
\contentdescription{
Discussion/Analysis [Interpretation of the results]
Could you combine approaches with each other or baselines to  combine strength, what are the strengths/weakness of each approach
}

\section{Discussion}

