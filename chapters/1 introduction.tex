\section{Motivation}
\subsection{Role of vocabulary in language acquisition}
Learning a second language involves many different skills, often categorized into listening, reading, speaking, and writing.
Another categorization may be vocabulary, grammatical skills, the ability to understand known words in various accents, understanding language when spoken at a fast speed.
One skill that is required for any of these if the knowledge of vocabulary in the target language.
A person with basic grammatical skills but no vocabulary has no ability to express themselves or understand anything which they hear around them.
On the other hand, a person familiar with rudimentary vocabulary but no grammatical knowledge may struggle with understanding complex sentences and sound unnatural when speaking, but can at least make sense of short phrases and express themselves.
Thus, basic knowledge of vocabulary is clearly one of the most essential skills for using a language.
This raises the question of which vocabulary should be learned first when starting out on the journey of language acquisition.


\subsection{Context-specific language learning}
Learners of languages are typically interested in one or more specific aspects of the language.
There is no such thing as an unbiased corpus than can fit every use case.
Decisions must be made how much focus is given to everyday conversation, academic writing, writing pertaining to business like job applications etc.
Many textbooks group vocabulary by topic, with a new topic being introduced with each lesson that student should ideally be able to converse in after completing the lesson.
However, this way of introducing vocabulary has several shortcomings:
\begin{itemize}
	\item Students spend time learning specific terms about the topic in one lesson while not learning even general vocabulary in other topics until much later.
	\item Knowing the words from previous lessons becomes a prerequisite for the more advanced material, especially because the terms from earlier lessons are used in example sentences for grammar.
	      Thus, learners interested in learning the use of the language in one context will have a hard time of skipping earlier, less pertinent lessons to them.
\end{itemize}

Since the advent of computer-aided natural language processing, methods have been suggested to computationally identify useful words:
Nation and Waring propose in an 1997 study \cite{nationVocabularySizeText1997} that the frequency with which a word appears in the target language should be used a metric for its importance to the learner, or utility, and thus teaching high-frequency words should be the focus when teaching beginners.
However, focusing on maximum-frequency words to achieve text coverage (e.g., knowledge of 90\% of words in a text) may not be as useful as one might at first think, as the most frequent words tend to be generic terms like “but”, “from”, “time”, “world” etc.
Knowing only these words is not sufficient for comprehending most texts.

The TF-IDF metric \cite{qaiserTextMiningUse2018} is often employed for finding the keywords in a document and thus a proxy for how important a word is for the overall meaning of the document.
It essentially is a frequency normalized against a larger, background corpus and expressed whether the usage frequency in the document is unusual.
Using TF-IDF as a measure for utility of a word in a given context is possible, but it suppresses words that may be generally useful.

Thus, there is a need for further exploration as to how word utility can be calculated, using modern NLP methods involving artificial intelligence where necessary.
Put more simply, this question may be reduced to:

\textbf{What order or words, when learned, gives the learner the best set of words to understand and communicate in the language as quickly as possible?}

\subsection{Examples of contexts and words}
Some examples for contexts that could be interesting to language learners include:

\begin{itemize}
	\item Reading Wikipedia articles about a specific field (computer science, literature, biographies)
	\item Watching movies
	\item Travel to a country where the target language is spoken
	\item Doing business with a company from a country where the target language is spoken
	\item Cultural exploration (literature, religion)
	\item Finding friends from other countries
\end{itemize}

The different contexts for which learners might be motivated to learn a language differ in how easily corpora can be obtained about to mine patterns from. Movie subtitles and Wikipedia articles are easily obtained from sites such as opensubtitles.org and wikipedia.org. The words that might be relevant for travel are not as easily obtained: One might imagine an ideal scenario to collect data, in which a statistically relevant group of people travelling to the destination to be examined are randomly selected and equipped with microphones and cameras before the travel. During travel, one could record their conversations, conversations with people around them, and materials they attempt to read to navigate their journey such as train schedules, descriptions of tours, restaurant menus, street signs, etc. Lacking the funds to conduct an experiment for every possible language, this paper is interested in finding a methodology to obtain data from readily available corpora and websites online that extracts relevant vocabulary from the source texts.
Depending on the context, we can think about which of the following English words might be likely to appear frequently in the texts:

\begin{itemize}
	\item Convert
	\item Cash
	\item Hug
	\item Dammit
	\item Y'all
	\item From
	\item Nineen eighty four
	\item Married
\end{itemize}

To examine a few examples: Words like “convert” occur frequently when looking at Wikipedia articles \footnote{see "Wikipedia" corpus 2016, drawn from one million lines on https://wortschatz.uni-leipzig.de/en/download/English}. “cash” is likely to be useful for travelers, but in most other contexts, it would not be as relevant. “Y’all” is almost never used in formal writing but used abundantly in everyday speech in the southern United Stated of America and South Africa. “From”, meanwhile, will be likely to be one of the most frequently used words regardless of context.

\section{Background}
\subsection{Requirements for calculating word utility from existing data}

\begin{description}
	\item [Large word corpora in many languages]
	\item [Hardware capabilities]
	\item [Software is able to process text on semantic level]
	      Text processing software and tools have existed since the 1980s, however before the advent of neural networks and other artificial intelligence methods, their capabilities were mostly bereft of any semantic understanding of the text processed: Using exclusively manually crated tools, it is possible to create a program which recognizes that \textit{table} and \textit{tables} derive from the same word, but much harder to recognize that there is any connection whatsoever between \textit{table} and \textit{chair}. The language processing capabilities of any human dwarf those of these early tools.
\end{description}

\subsection{Developments in Natural Language Processing}
\begin{description}
	\item [Sophisticated tokenizers]
	\item [Wordnets]
	\item [Neural networks trained in NLP tasks]
	\item [Explainable AI]
\end{description}

This paper aims to exploit these significant developments to gain insights into which words can provide second language learners with the most utility.

\section{Method of this paper}
It is evident that recent tools based on Artificial Intelligence are much better-equipped for many tasks in the realm of Natural Language Processing than rule-based tools.
At the same time, it is less clear how exactly these networks achieve the results they do.
Fortunately, there is a branch in the field of Artificial Intelligence called Explainable Artificial Intelligence which aims at explaining the outputs of AI models.
Thus, it may be possible to harness the evident intelligence off AI models to simulate a human, to find out which words have the largest impact on the model's performance.

\subsection{Word extraction methods}
The approaches for approximating utility with an automatically computable metric which this works aims to compare include:
\subsubsection{Traditional}
\begin{description}
	\item [Raw frequency of words in corpus]
	      A simple ordering of words by how often they appear in a corpus.
		\item[Frequency with stopwords filtered out]
			The same as frequency, but filtering out known stopwords from the resulting lists.
	\item [TF-IDF]
	      How often the words appear in a target corpus but divided by their frequency in a more generic corpus.
	      This metric is typically used to employ the most relevant words in documents for identifying keywords that express best its core topic.
\end{description}
\subsubsection{AI-simulated learner}
\begin{description}
	\item [Performance difference of AI for NLP tasks]
	      Here, a Large Language Model (LLM) or a more specific language processing model is made to run NLP tasks such as text summarization, sentiment detection or question-answering.
	      To find out which words help the AI model the most in performing its tasks, words are methodically omitted from texts and the AI’s performance is recorded.
	      This metric attempts to approximate utility by finding words which, when missing, cause the greatest performance loss in the NLP tasks.
		  Evaluation metrics like Shapley values \cite{wangShapleyExplanationNetworks2021} may be used to measure the impact of missing words
	\item [Transformer attention]
		The transformer architecture is based on a mechanism called \textit{self-attention}.
		It allocates the neural network's processing to important parts of the input and thus provides some degree of explainability "out of the box".

	\item [Difference in internal vector representation for AI reading text]
	      This approach words similarly to the above involving an AI model, but instead of measuring the changes in the quality of its output, it measures how much changing the input to the model changes its the internal vector state: AI stores data in vector format, and when performing NLP tasks on texts, there is an internal vector representation.
	      By using various distance metrics, it may be possible to find out which words have the greatest impact on the model’s understanding of a text.
	      Most of these approaches can be done both for individual words and word sequences (n-grams).
	      While individual words are the easiest to examine, sometimes n-grams are insightful for finding sequences of words whose meaning is more than the sum of their parts (idioms and collocations) and which therefore must be learned in separately from their constituents (meaningful English n-grams include e.g. “kick the bucket”, “such that”, “such as”).

	      This also raises the question of what is considered a “word”.
	      A phrase like “such as” can be considered two words if the definition of a word is simply “something separated by a space” or one word if the definition is “a phrase whose meaning cannot be arrived at trivially from knowing the definition of its parts”.
	      In Natural Language Processing, tokenizers break down texts into words, but they typically use the first definition for a word in the case of English.
	      Many non-European language do not use spaces in their spelling (e.g. Japanese, Mandarin Chinese) or use spaces to separate a different unit of text (syllables in Vietnamese, sentences in Thai), making this definition of a word unpractical.
	      In most languages, words can appear in various different forms: Verbs in Spanish are conjugated according to the time and originator of an action, Nouns in German are declined depending on their number and grammatical case.
	      This adds another variable for compiling word lists: Whether the list should consider any different combination of letters as a different word, or whether different forms of the same headword should be viewed as only one word.
\end{description}

Key technologies employed include therefore:
\begin{itemize}
	\item Tokenizers
	\item Lemmatizers
	\item Translators
	\item AI models to perform NLP tasks
\end{itemize}

