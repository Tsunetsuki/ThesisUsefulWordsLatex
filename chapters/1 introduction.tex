\todo {Write Intro}

Put succinctly, we can pose this as a challenge of finding a computational method that can answer the question:

\begin{quote}
	\textit{Which words provide the most language understanding in a given linguistic context?}
\end{quote}

We can subdivide the overarching goal of word utility evaluation into three related, but not quite equivalent sub-goals:
\begin{itemize}
	\item Evaluating the utility of a single word.
	\item Evaluating the utility of an (ordered) list of words to learn.
	\item Generating maximally useful (ordered) lists of words to learn.
\end{itemize}

The first and third goal are directly related, in that evaluating the utilities of words in a text trivially generates a useful list of vocabulary if we compile the list by aligning the words in order of their descending utility. 
Evaluating the utility of an ordered list of vocabulary is conceptually further removed from the other two sub-goals, but nevertheless, its implementation shares many aspects with them.
For this reason, many chapters in this work contribute to all of these goals equally.
Therefore, to avoid redundancy, we refer to the overarching goal of this work as "word utility evaluation" throughout the following chapters - which is meant to include these three sub-tasks.




\section{Motivation}
\subsection{Role of Vocabulary in Language Acquisition}
Learning a second language involves many different skills, often categorized into listening, reading, speaking, and writing.
Another categorization may be vocabulary, grammatical skills, the ability to understand known words in various accents, understanding language when spoken at a fast speed.
One skill that is required for any of these if the knowledge of vocabulary in the target language.
A person with basic grammatical skills but no vocabulary has no ability to express themselves or understand anything which they hear around them.
On the other hand, a person familiar with rudimentary vocabulary but no grammatical knowledge may struggle with understanding complex sentences and sound unnatural when speaking, but can at least make sense of short phrases and express themselves.
Thus, basic knowledge of vocabulary is clearly one of the most essential skills for using a language.
This raises the question of which vocabulary should be learned first when starting out on the journey of language acquisition.
This work attempts to contribute to answering this question.

\todo{Mention various methods of finding useful vocabulary from viewpoint of learner: textbooks, apps, etc.}

\subsection{Context-specific Language Learning}
Learners of languages are typically interested in one or more specific aspects of the language.
There is no such thing as an unbiased corpus than can fit every use case.
Decisions must be made how much focus is given to everyday conversation, academic writing, writing pertaining to business like job applications etc.
Many textbooks group vocabulary by topic, with a new topic being introduced with each lesson that student should ideally be able to converse in after completing the lesson.
However, this way of introducing vocabulary has several shortcomings:
\begin{itemize}
	\item Students spend time learning specific terms about the topic in one lesson while not learning even general vocabulary in other topics until much later.
	\item Knowing the words from previous lessons becomes a prerequisite for the more advanced material, especially because the terms from earlier lessons are used in example sentences for grammar.
	      Thus, learners interested in learning the use of the language in one context will have a hard time of skipping earlier, less pertinent lessons to them.
\end{itemize}


(cut segment about frequency and TF-IDF here)

Thus, there is a need for further exploration as to how word utility can be calculated, using modern NLP methods involving artificial intelligence where necessary.
Put more simply, this question may be reduced to:

\textbf{What order or words, when learned, gives the learner the best set of words to understand and communicate in the language as quickly as possible?}

\subsection{Examples of Contexts and Words}
Some examples for contexts that could be interesting to language learners include:

\begin{itemize}
	\item Reading Wikipedia articles about a specific field (computer science, literature, biographies)
	\item Watching movies
	\item Travel to a country where the target language is spoken
	\item Doing business with a company from a country where the target language is spoken
	\item Cultural exploration (literature, religion)
	\item Finding friends from other countries
\end{itemize}

The different contexts for which learners might be motivated to learn a language differ in how easily corpora can be obtained about to mine patterns from. Movie subtitles and Wikipedia articles are easily obtained from sites such as opensubtitles.org and wikipedia.org. The words that might be relevant for travel are not as easily obtained: One might imagine an ideal scenario to collect data, in which a statistically relevant group of people travelling to the destination to be examined are randomly selected and equipped with microphones and cameras before the travel. During travel, one could record their conversations, conversations with people around them, and materials they attempt to read to navigate their journey such as train schedules, descriptions of tours, restaurant menus, street signs, etc. Lacking the funds to conduct an experiment for every possible language, this work is interested in finding a methodology to obtain data from readily available corpora and websites online that extracts relevant vocabulary from the source texts.
Depending on the context, we can think about which of the following English words might be likely to appear frequently in the texts:

\begin{itemize}
	\item Convert
	\item Cash
	\item Hug
	\item Dammit
	\item Y'all
	\item From
	\item Nineteen eighty-four
	\item Married
\end{itemize}

To examine a few examples: Words like “convert” occur frequently when looking at Wikipedia articles \footnote{See "Wikipedia" corpus 2016, drawn from one million lines on \url{https://wortschatz.uni-leipzig.de/en/download/English}}. The word "cash" is likely to be useful for travelers, but in most other contexts, it would not be as relevant. “Y’all” is almost never used in formal writing but used abundantly in everyday speech in the southern United Stated of America and South Africa. “From”, meanwhile, will be likely to be one of the most frequently used words regardless of context.

\section{Challenges and Contributions}
\todo{Mention how many languages our approach currently supports }

In this work, we make the following contributions:

\begin{itemize}
	\item A workable definition of word utility (Section \ref{sec:utility}).
	\item The development of a framework for evaluating and generating vocabulary lists, by evaluating the utility of words to understand the meaning of a given text (Chapter \ref{ch:approach}).
	\item The analysis and selection of appropriate proxy tasks, corpora, and XAI methods, leading the implementation of this approach (Chapter \ref{ch:implementation}).
	\item Analysis of the resulting vocabulary lists in comparison with each other, as well as frequency-based baseline methods (Chapter \ref{ch:evaluation}).
\end{itemize}

A major challenge in making computational approaches which generate maximally useful lists of vocabulary is that, to the best of our knowledge, no automatic process has yet been proposed which can measure how useful the words in a vocabulary list are. 
Thus, in order to evaluate the vocabulary lists produced by our generation approach, we develop our own evaluation method, which is based on similar ideas as our approach for list generation.
However, we must also assess whether our evaluation approach itself is reliable, which is necessarily manual and subjective for lack of other evaluation measures.

A general theme of this work is multilinguality --- we attempt to make our implementation applicable to the largest number of languages possible.
For instance, we seek out sources of text to use as input to our data pipeline where many languages are available and labeled.
This criterion limits the resources available.
Another challenge in our particular word utility evaluation approach was finding a mechanism to automatically measure an AI model's understanding of texts, as this understanding is used to calculate a word's utility.
For this, we let the AI model perform a task on the text, similar to a language exam in a school.
However, finding tasks that require a broad language understanding to be performed but which can also be reliably evaluated automatically was a major difficulty.

% Inherent challenges of text-based language analysis include:
% \begin{itemize}
% 	\item ambiguity
% 	\item Words having multiple different meanings (polysemy). E.g., \textit{can} can be a verb or a vessel.
% \end{itemize}

% \subsection{Contributions}
% [Motivation: Useful vocabulary -> Need approach to evaluate utility of word]
% [How this work address the challenges: Simulate human trying to understand texts with AI]
% [Distinguish between passive and active utility]


\section{Outline of Work}

The following gives an overview of this work from a top-level perspective.

We begin by placing the work in context in Chapter \ref{ch:background}, by first establishing how to the concept of "utility" is used in existing linguistic research about vocabulary list compilation, and explaining further basic concepts such as a \textit{linguistic context}.
With the use of these basic terms and concepts, the aim of this thesis is stated more precisely, followed by an introduction of the main fields of research involved in our solution to achieve this aim.
Finally, we include a survey about concrete methods which are currently used to compile vocabulary lists, and how this work aims at improving them with the use of AI models.

Chapter \ref{ch:approach} describes in detail our theoretical approach to both evaluating and generating vocabulary lists.
To this end, we first transform our problem into a mathematical one.
We then describe an empirical approach for evaluating how well a vocabulary list helps a learner to understand a given context, and how we automate the approach by using \AI\ models to simulate a learner and corpora to simulate linguistic concexts.
With the evaluation approach as a basis, we develop a second approach for generating vocabulary lists, using methods from the field of Explainable AI.

The technical implementation for both approaches is detailed in Chapter \ref{ch:implementation}:
After having established the principal components to the approaches in Chapter \ref{ch:approach}, we discuss the choice of these components, namely corpora, AI models, and Explainable AI methods:
For each component, we first describe desiderata, and then make a number of concrete choices which were used in our experiments.


Chapter \ref{ch:evaluation} discusses the evaluation of the vocabulary lists generated by our list generation approach.
For this, we mainly utilize our own evaluation approach described in Chapter \ref{ch:approach}, but also make several subjective observations on the perceived utility of the lists from a human intuitive perspective.
We evaluate how well the approaches perform on two different types of datasets, and how much time they require, using frequency-based approaches, which predominate the literature on vocabulary list compilation, as baselines.
Additionally, we analyze how similar the lists are to one another, independent of their performance.

Finally, Chapter \ref{ch:outlook} outlines potential improvements to our approach to word utility evaluation, which could not be implemented within the scope of this work due to time constraints.





